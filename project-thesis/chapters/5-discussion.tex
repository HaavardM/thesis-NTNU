\chapter{Discussion \& Conclusion}\label{chap:discussion}

\section{Assuming conjugate priors}\label{sec:discuss_conjugate_priors}
The example in the previous chapter was only using an illustrative model in order to demonstrate the benefit of Bayesian Inference. The dataset available did not contain nearly enough data to fully identify all parameters, but through the use of prior distribution when designing the model, it was still able to learn from available observations. However, some of the modelling assumption were only possible without assuming conjugate priors.

The mean of the Gaussian mixture components were assumed a priori to be an angle between $(0, \pi)$. Using a scaled Beta distribution, it was easy to express that the mean should lie in $(0, \pi)$ and slightly prefer values closer to $\frac{pi}{2}$. Using the conjugate prior Gaussian distribution would be challenging in this case, as the prior then would support values outside of $(0, \pi)$. This is where approximate methods such as $\acrshort{vi}$ and \acrshort{mcmc} really shine as they allow incorporation of prior knowledge using arbitrary distributions.  

\section{Choice of surrogate density for \acrshort{vi}}
It is important to emphasize that using a Gaussian mean-field approximation as surrogate density in the previous example is a strong assumption. For the illustrative example it was sufficient to demonstrate how \acrshort{vi} can be used for inference in arbitrary models, but at a loss of precision. However, in a real world application, the surrogate density should be more carefully selected in order to retain as much of the information in the posterior as possible. How to select the best surrogate density will depend on the true posterior distribution. A broader set of surrogate densities should be explored and analytical methods should be used wherever possible to reduce the loss of information due to approximations. \acrshort{mcmc} could also aid the choice of a surrogate density during the design phase.

\section{Consistency}
For use in a \acrshort{colav} system, an important trait is consistency. An incorrect intention prediction could lead to dangerous situations, and it is important that the model never expresses certainty about incorrect values. The results from multiple simulations in \cref{fig:example_mc_mcmc} and \cref{fig:example_mc_vi} show how the results vary between different realizations of the dataset. While it is disappointing to see incorrect predictions, it is more worrying how the uncertainty is low in some simulations even if the predictions are off. These results would simply not be acceptable for real-world use, as the uncertainty does not reflect the true error. This is especially bad for \acrshort{vi} as it is unable to express the uncertainty. For \acrshort{mcmc} it looks better, with most of the predicted posteriors embracing the true value, though some predictions are still too far off to be acceptable. 

\section{Computational Complexity}
The major drawback for \acrshort{mcmc} is the computational complexity. Each simulation in the previous chapter took more than 8 minutes to compute using \acrshort{mcmc}, making it slow and tedious to work with. While $N=30000$ samples for each of the $6$ chains may sound like a lot, it cannot guarantee that the chains actually have converged to the true posterior distribution, and the result may turn out to be invalid. Further, considering the $10000$ burn-in samples and thinning, each chain only result in $10000$ usable samples. While there are different ways to inspect whether a chain has converged, these methods are usually complicated to use and are often not able to actually guarantee convergence. A common approach seems to be to rely on the "Law of large numbers" by simply running the chains for as long as one can afford, throw away a fixed number of samples for burn-in and then hope for the best. This might be a valid approach if one can afford the wait, but makes these methods unfit for \acrshort{colav} applications.  

\acrshort{vi} is a lot more efficient, and it scales well with the amount of data. In the examples, it took only seconds to fit the surrogate density, and the results were usually comparable to \acrshort{mcmc}, especially when taking MAP predictions into consideration. Looking back at \cref{fig:example_mc_vi}, the MAP predictions are rather evenly distributed around the true value and comparable to the much slower \acrshort{mcmc} method. It is however unable to accurately estimate the true uncertainty and is clearly limited by the Gaussian approximation.    


\section{When to use which method?}

\subsection{When all variables are discrete}
In the case of \acrshort{pgm}s with only discrete variables, exact inference methods are often applicable and should be preferred. The challenges of Bayesian inference in such networks usually boils down to computational complexity when the number of variables increases and in which order the computations should be made. Exact methods such as \acrshort{bp} are in most cases the most appropriate choice, though some structures (in the case of loops) cannot guarantee an optimal solution. For really large problems where the number of hypotheses becomes intractable, approximate methods can still be relevant. 

\subsection{When some variables are continuous}
When some variables are continuous, the problem usually gets a bit more complicated. If all parent-child pairs can be expressed using conjugate priors, such as the case of Gaussian Belief Networks, exact methods can still be used. However, requiring the use of only conjugate priors may in many cases be too restrictive as already discussed in \cref{sec:discuss_conjugate_priors}. Exact methods may still be used on parts of a model where the assumptions are valid and fall back to approximate methods for more complicated parts of the model.

The most straight-forward approach will be to use \acrshort{mcmc} methods, due to their inherent simplicity. These methods allow sampling from arbitrarily complex models as long as the target probability can be evaluated. It can in most cases provide asymptotic guarantees that the samples are from the true posterior distribution, though only given a large amount of samples. How many samples that are considered "enough" is difficult to say, and it usually requires manual interpretations of the results in order to verify convergence. Using \acrshort{mcmc} in autonomous systems may therefore be challenging unless the model is sufficiently simple, in which case other methods may still be preferable. Due to the stochastic nature of sampling methods, \acrshort{mcmc} is likely to be a poor choice for problems where deterministic behaviour is valued, as the behaviour is only guaranteed in the limit.  

Though it requires more work, \acrshort{vi} will be a better option in cases where time is important. By posing the problem as an optimization problem rather than relying on sampling, variational inference can give deterministic behaviour as well as drastically speed up inference when compared to \acrshort{mcmc}. In the examples above, it took less than $13$ seconds to optimize with a fixed number of iterations, and by inspecting the ELBO it becomes obvious that the training time can be further reduced by early stopping when the learning rate starts to diminish. 
However, \acrshort{vi} requires manual selection of a good surrogate density, and the choice of a bad surrogate may lead to poor results due to invalid assumptions. \acrshort{vi} does therefore by itself not provide any guarantees on the correctness of the results as it will always be limited by the assumptions and approximations used by the surrogate density. In the case of complex posterior densities, it may also be challenging to find a proper functional representation of the distribution. As already seen in \cref{fig:example_mcmc_posterior}, the true posterior found by \acrshort{mcmc} does not resemble any known probability distribution. 

A lot of research is currently focusing on how to apply \acrshort{vi} on more complicated models without the need of error-prone calculations or strict assumption. Methods such as variational message passing allows for more complicated surrogate densities in order to retain the interaction between variables in the model, by combining \acrshort{bp} with \acrshort{vi} \cite{winnbishop}. 

In practice, both methods are likely needed. During the design-phase, \acrshort{mcmc} methods can be used to "blindly" sample from the true posterior in order to aid the selection of a proper surrogate density. In a deployed system, \acrshort{vi} can then be used to efficiently approximate the true posterior without loosing too much information to invalid assumptions.

\section{Conclusion}
Bayesian Networks allow a system to combine prior knowledge from human experts, predefined rules and intuition by encoding such information into prior distributions. Bayesian Networks further allow intuitive reasoning about assumed relationships between variables, where people without a statistical background can participate and reason about the design of a model. It is therefore a good framework to use when designing intention models for \acrshort{colav}, as prior knowledge can be used to improve the model when available data is sparse.

Performing exact Bayesian inference was found to be challenging due to the mathematical complexity. Exact inference methods usually rely on some rather restrictive assumptions, which can negatively impact the ability to express prior knowledge in a model. Approximate methods, such as \acrshort{mcmc} and \acrshort{vi} could circumvent these assumption and be used in arbitrary models, though as some loss of precision. On a fictitious example, \acrshort{mcmc} was found to be slow and tedious to work with, but gave accurate results. \acrshort{vi} was found to run much faster, but at a severe loss of precision due to the choice of surrogate density. Both methods are therefore important tools when working with Bayesian Networks, though \acrshort{vi} are likely more relevant for use in \acrshort{colav} systems, due to the lower computational complexity. 

\section{Future Work}
This thesis did not attempt to develop any realistic intention model, but rather explore possible ways of performing inference. Few assumptions were therefore made about the model in order to not impose any restrictions when designing an intention model in the future. This leaves a lot of possible improvements and work left for the future.

\begin{enumerate}
    \item Design a realistic inference model using available data from real vessels. This will allow further assumptions about the model to be made. 
    \item Attempt to use well-known \acrshort{pgm} model structures, such as Hidden Markov Models. 
    \item More careful choice of surrogate density to improve the performance of \acrshort{vi}.
    \item Explore the effects of using structured mean field to retain interactions in the approximate posterior distribution when using \acrshort{vi}. 
    \item Combine exact inference with approximate methods.
\end{enumerate}



