\chapter{Neccessary Theoretical Background}\label{chap:theory}

\section{The Multivariate Gaussian Distribution}
The Gaussian Distribution is one one the most used distributions in statistics \cite{murphy} and generalizes well into the multivariate case. The pdf for the $D$ dimensional multivariate Gaussian is given by \cref{eq:multivariate_gaussian} \cite{murphy}.
\begin{equation}\label{eq:multivariate_gaussian}
    \mathcal{N}(\boldsymbol{x} \; | \; \boldsymbol{\mu}, \boldsymbol{\Sigma}) \triangleq \frac{1}{(2 \pi)^{D/2} |\boldsymbol{\Sigma} | ^{1/2}} \exp \bigg[- \frac{1}{2} (\boldsymbol{x} - \boldsymbol{\mu})^\intercal \boldsymbol{\Sigma}^{-1}(\boldsymbol{x} - \boldsymbol{\mu})\bigg]
\end{equation}
\subsection{Marginalization and conditioning}
Consider a joint multivariate Gaussian distribution for two (possibly vector-valued) variables $\boldsymbol{x}$ and $\boldsymbol{y}$.
\begin{equation}
    p(\boldsymbol{x}, \boldsymbol{y}) = N\bigg(\begin{bmatrix}
        \boldsymbol{x} \\ \boldsymbol{y}
    \end{bmatrix} \; \bigg| \; \begin{bmatrix}
        \boldsymbol{\mu}_x \\ \boldsymbol{\mu}_y
    \end{bmatrix}, \begin{bmatrix}
        \boldsymbol{\Sigma_{xx}} &
        \boldsymbol{\Sigma_{xy}} \\
        \boldsymbol{\Sigma_{yx}} &
        \boldsymbol{\Sigma_{yy}}
    \end{bmatrix}\bigg)
\end{equation}
The marginals are then given by \cref{eq:multivariate_gaussian_marginal} and can simply be calculated by picking the corresponding values from $\boldsymbol{\mu}$ $\boldsymbol{\Sigma}$, i.e. no calculations needed. 
\begin{align}\label{eq:multivariate_gaussian_marginal}
    \begin{split}
    p(\boldsymbol{x}) &= \mathcal{N}(\boldsymbol{x} \; | \; \boldsymbol{\mu}_x, \boldsymbol{\Sigma}_{xx})\\
    p(\boldsymbol{y}) &= \mathcal{N}(\boldsymbol{y} \; | \; \boldsymbol{\mu}_y, \boldsymbol{\Sigma}_{yy})
    \end{split}
    \end{align}
 The posterior conditional is given by \cref{eq:multivariate_gaussian_conditional}. These equations will be used extensively throughout this thesis.
 \begin{tcolorbox}[title={Posterior Conditional Distribution}]
 \begin{subequations}\label{eq:multivariate_gaussian_conditional}
 \begin{align}
    p(\boldsymbol{x} | \boldsymbol{y}) &= \mathcal{N}(\boldsymbol{x} \; | \; \boldsymbol{\mu}_{x|y}, \boldsymbol{\Sigma}_{x|y})\\
    \boldsymbol{\mu}_{x|y} &= \boldsymbol{\mu}_x + \boldsymbol{\Sigma}_{xy}\boldsymbol{\Sigma}_{yy}^{-1}(\boldsymbol{y} - \boldsymbol{\mu}_y)\\
    \boldsymbol{\Sigma}_{x|y} &= \boldsymbol{\Sigma}_{xx} -\boldsymbol{\Sigma}_{xy}\boldsymbol{\Sigma}_{yy}^{-1}\boldsymbol{\Sigma}_{yx}
 \end{align}
 \end{subequations}
 \end{tcolorbox}


\section{Introduction to Gaussian Processes}

This introduction is heavily inspired by \cite{rasmussen}.

A \acrfull{gp} can formally be defined as Definition \ref{def:gp}.

\newtheorem{gp_def}{Definition}
\begin{gp_def}\label{def:gp}
A Gaussian Process is a collection of random variables, any finite number of which has a joint Gaussian Distribution.
\end{gp_def}

In this thesis we will use a more specific definition in order to interpret \acrshort{gp}'s as a statistical distribution over functions. A \acrshort{gp} for a random function $f \triangleq f(\boldsymbol{x})$ is fully specified by its mean function $m(\boldsymbol{x})$ and covariance function $k(\boldsymbol{x}, \boldsymbol{x}')$.

\begin{equation}\label{eq:gp}
    f \sim GP(\;m(\boldsymbol{x}), \; k(\boldsymbol{x}, \boldsymbol{x}')\;)
\end{equation}

This interpretation of a \acrshort{gp} might seem a bit odd at first. The key observation is that the marginal distribution $p(\boldsymbol{x})$ of a multivariate Gaussian distribution $p(\boldsymbol{x}, \boldsymbol{y})$ is another Gaussian distribution that is completetly independet of $\boldsymbol{y}$, as expressed in \cref{eq:gaussian_marginal}. Any variables that is not of intereset can therefore easily be marginalized away, leaving only the subset of variables we care about.

\begin{equation}\label{eq:gaussian_marginal}
    p(\boldsymbol{x}) = \int_{\boldsymbol{y}} \mathcal{N} \bigg(\begin{bmatrix}
        \boldsymbol{x} \\ \boldsymbol{y}
    \end{bmatrix}\; \bigg| \begin{bmatrix}
        \boldsymbol{\mu_x} \\ \boldsymbol{\mu_y}
    \end{bmatrix}, \; \begin{bmatrix}
        \boldsymbol{\Sigma_{xx}} & \boldsymbol{\Sigma_{xy}} \\
        \boldsymbol{\Sigma_{yx}} & \boldsymbol{\Sigma_{yy}} \\
    \end{bmatrix} \bigg) d\boldsymbol{y} = \mathcal{N} \big( \boldsymbol{x} \;\big| \; \boldsymbol{\mu_x}, \boldsymbol{\Sigma_{xx}}\big)
\end{equation}

Any \acrshort{gp} by Definition \ref{def:gp} can therefore be thought of as the finite marginal distribution of an infinite Gaussian Distribution, jointly describing the values of $f$ at all possible inputs $\boldsymbol{x}$. In the end, a \acrshort{gp} is nothing more than a joint Gaussian Distribution with a fancy interpretation.

\subsection{Introduction to kernels}
The covariance function $k(\boldsymbol{x}, \boldsymbol{x}')$ determines the similarity between two different points $\boldsymbol{x}$ and $\boldsymbol{x}'$. These covariance functions will in this theis be referred to as a \textit{kernel}, which maps the input space to a \textit{feature-space} \cite{rasmussen}. The key idea behind kernels is that the kernels may in many cases be simpler to compute than the input vectors themselves if the input-space is complex. The output of the kernel is a value describing the similarity (i.e. covariance) between the two inputs. Different types of kernels will be discussed in greater detail in \cref{sec:kernels}.

The kernel must be positive definite in order to produce a valid covariance matrix, which requires that

\begin{equation}
    k(\boldsymbol{x}, \boldsymbol{x}') = k(\boldsymbol{x}', \boldsymbol{x})
\end{equation}

The resulting covariance matrix $K(X, X)$ is the result of calling $k(\cdot, \cdot)$ on all pairs on inputs, i.e.
\begin{equation}
    K(X, X)_{ij} = k(\boldsymbol{x}_i, \boldsymbol{x}_j) \quad \forall i, j
\end{equation}

\subsection{Conditioning}
So far we have only specified the prior distribution for $f$. As a \acrshort{gp} is by definition a multivariate Gaussian Distribution, the posterior conditional in \cref{eq:multivariate_gaussian_conditional} can be used to condition $f$ on observed values. 
As an example, we use a simple \acrshort{gp} for $f$ where mean $m$ and kernel $k$ is defined in \cref{eq:theory_simple_gp}. The \acrshort{rbf} kernel used will be introduced in greater detail in \cref{sec:kernels_rbf}.
\begin{align}\label{eq:theory_simple_gp}
    m(x) &= 0 & k(x, x') &= \exp\bigg(-\frac{|| x - x'||^2}{2}\bigg)
\end{align}
Before any observations are made we can only rely on prior information as shown in \cref{fig:gp_prior}. From a Bayesian perspective, we want to update our prior beliefs after observing $f_* \triangleq f(x_*)$ at different inputs $x_*$ to get the posterior belief $f \; | \; f_*$.

The joint distribution of $f$ and $f_*$ is given by 

\begin{equation}
    p(f, f_*) = \mathcal{N}\bigg(\begin{bmatrix}
        f \\ f_*
    \end{bmatrix} \; \bigg| \begin{bmatrix}
        m(x) \\ m(x_*)
    \end{bmatrix},  \begin{bmatrix}
        K(X, X) & K(X, X_*) \\ K(X_*, X) & K(X_*, X_*)
    \end{bmatrix}\bigg)
\end{equation}

The posterior distribution $f | f_*$ can easily be computed using the closed form solution in \cref{eq:multivariate_gaussian_conditional}.
\begin{subequations}
\begin{align}
    p(f | f_*) &= \mathcal{N}(f \; | \; \boldsymbol{\mu}_{f|f_*}, \boldsymbol{\Sigma}_{f|f_*})\\
    \boldsymbol{\mu}_{f|f_*} &= m(x) + K(X, X_*) \; K(X_*, X_*)^{-1} \; (f_* - m(x_*))\\
    \boldsymbol{\Sigma}_{f|f_*} &= K(X, X) - K(X, X_*)  K(X_*, X_*)^{-1} \; K(X_*, X)
\end{align}
\end{subequations}

In practice, computing the inverse $K(X_*, X_*)$ quickly becomes expensive and error-prone for increasing number of samples. To avoid numerical instability, the \textit{Cholesky Decomposition} is usually preferred. The Cholesky Decomposition forms a new lower-triangular matrix $L$ such that $K = L L^\intercal$, and can be seen as the matrix equivalent of the square root.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/introduction-gp/prior.png}
        \caption{Prior}
        \label{fig:gp_prior}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/introduction-gp/posterior.png}
        \caption{Posterior after observing the function at two different inputs (black dots).}
        \label{fig:gp_posterior}
    \end{subfigure}
    \caption{Simple Gaussian Process example with zero-mean and \acrshort{rbf} kernel with unit variance. The red line is the mean, while the red area is the $95\%$ confidence interval.}
\end{figure}

\section{Model Selection}
\section{Kernels}\label{sec:kernels}

\subsection{Radial Basis Function}\label{sec:kernels_rbf}
\section{Multivariate Gaussian Process}
\section{Computational Complexity}
\section{Approximate methods}
\subsection{Variational Inference}
\subsection{Markov Chain Monte Carlo}
