\chapter{Learning trajectories directly from data}
Perhaps the most direct approach is to use a \acrshort{gp} to model a function $f: \mathcal{R}^5 \to \mathcal{R}^2$, mapping a ships current position $\boldsymbol{p}_t$, \acrshort{cog} $\psi_t$ and \acrshort{sog} $v_t$ to a position at time $t+\tau$. The method is formulated in mathematical terms in \cref{eq:gp_direct}. The model can then be queried about likely future positions by evaluating $\boldsymbol{f}$ with the desired initial conditions and time horizon, and then condition the \acrshort{gp} on close trajectories. 
The method boils down to using the \acrshort{gp} kernels to find trajectories with similar initial conditions and use those to predict future positions through clever interpolation and averaging.

\begin{subequations}\label{eq:gp_direct}
\begin{align}
    \boldsymbol{f}(\boldsymbol{x}) &= \boldsymbol{p}_{t+\tau} \label{eq:gp_direct_f}, \quad \boldsymbol{x} = \begin{bmatrix} \boldsymbol{p}_t & \psi_t & v_t & \tau\end{bmatrix}\\
    \boldsymbol{f}(\boldsymbol{x}) &\sim \text{GP}(\boldsymbol{m}(\boldsymbol{x}), K(\boldsymbol{x}, \boldsymbol{x}))\label{eq:gp_direct_f_dist}
\end{align} 
\end{subequations}

The benefits of this approach include:
\begin{description}
    \item[Continuous formulation] The model directly models the position at any time $t+\tau$ and do not require discretization. 
    \item[Simple Problem formulation] The model requires few components.
    \item[Easy incorporation of available AIS data] The reported \acrshort{cog} and \acrshort{sog} can easily be incorporated into the similarity measure, utilizing more of the available data.
\end{description}

However, the model is exclusively data-driven and makes it more challenging to incorporate model information. The high number of input variables also makes it more difficult to interpret how the model makes a prediction. In turn, this makes tuning more challenging. This model further assumes that the trajectory is jointly Gaussian, making it harder to describe branching trajectories.




\section{Choice of kernels}

\section{Implementation}
The model is implemented in \textit{GPFlow} \cite{GPflow2017}, a \acrshort{gp} framework built on the well-known \textit{tensorflow} \cite{tensorflow2015-whitepaper} library. GPFlow has several implementations of \acrshort{gp}s, including both excact implementation and \acrshort{svgp} we have discussed earlier. We will here utilize the \acrshort{svgp} implementation to learn a sparse approximation of \cref{eq:gp_direct} direcly from all available data. 

For optimization, the stochastic gradient optimizer \textit{ADAM}, which should familiar for anyone working with Neural Networks. With only a few lines-of-code, the \acrshort{elbo} can be optimized in order to get a variational approximation of the true posterior distribution.

\section{Comparison to neural networks}


