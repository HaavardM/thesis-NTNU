\chapter{Learning trajectories directly from data}
The first method proposed is to use a \acrshort{gp} to model a function $f: \mathcal{R}^5 \to \mathcal{R}^2$, mapping a ships current position $\boldsymbol{x}_0$, \acrshort{cog} $\psi_0$ and \acrshort{sog} $v_0$ to a position at timestamp $\tau$ in the future. The method is formulated in mathematical terms in \cref{eq:gp_direct}. 

\begin{subequations}\label{eq:gp_direct}
\begin{align}
    \boldsymbol{f}(\boldsymbol{\eta}) &= \boldsymbol{x}_{\tau} \label{eq:gp_direct_f}, \quad \boldsymbol{\eta} = \begin{bmatrix} \boldsymbol{x}_0 & \psi_0 & v_0 & \tau\end{bmatrix}\\
    \boldsymbol{f}(\boldsymbol{\eta}) &\sim \text{GP}(\boldsymbol{m}(\boldsymbol{\eta}), K(\boldsymbol{\eta}, \boldsymbol{\eta}))\label{eq:gp_direct_f_dist}
\end{align} 
\end{subequations}

The model could then be queried about likely future positions by evaluating $\boldsymbol{f}$ with the desired initial conditions and time horizon by directly applying \cref{alg:gp_prediction}. 

The conceptual benefits of this formulation include:
\begin{description}
    \item[Continuous formulation] The model directly models the position at any time $\tau$ and does not require discretization. It can therefore be queried for any continous timestamp $\tau$.
    \item[Conceptually easy] This formulation directly expresses the unknown trajectory, making it conceptually simple to understand.
    \item[Simple Problem formulation] The model requires few components and builds directly on the \acrshort{gp}s introduced in \cref{chap:theory}.
    \item[Easy incorporation of available AIS data] The reported \acrshort{cog} and \acrshort{sog} can easily be incorporated into the similarity measure, utilizing more of the available data.
\end{description}

\section{Method}
While simple, the formulation in \cref{eq:gp_direct} still leads to quite a lot of complexity due to the number of input parameters. The method formulation should in theory be able to express a wide range of trajectories, but in practice the method is limited by the computational complexity.

\subsection{Excact GP using only a subset of the data}
The kernel matrix $K$ for all available data will be approximately sparse. Only a subset of the \acrshort{ais} data is actually relevant for any given input query. Trajectories with very different initial conditions than the query input should be safe to ignore, leaving only a subset of relevant trajectories to use for training. This way, the exact \acrshort{gp} formulation can still be used.
In this thesis, the training trajectories are selected from the following requirements:
\begin{enumerate}
    \item The initial position of the trajectory must be close to the queried position $\boldsymbol{x}_0$. In this thesis, a fixed threshold $\Delta \boldsymbol{x}$ is used.
    \item The initial \acrshort{cog} must be close to the queried heading $\mathcal{X}_0$. In this thesis, a fixed threshold $\Delta \mathcal{X}$ is used. 
    \item The initial \acrshort{sog} must be close to the queried velocity $v_0$. In this thesis, a fixed threshold $\Delta v$ is used.
\end{enumerate} 

This is a very simplistic approach to clustering, as new \acrshort{gp}s needs to be trained for each query. A possible extensions to this work is to perform clustering beforehand, and train a \acrshort{gp} for each cluster instead. 

\section{Implementation}
The excact \acrshort{gp} formulation was implemented using the \texttt{GaussianProcessRegressor} from the popular Python library, \textit{sciki-learn} \cite{scikit-learn}. The library support all kernels introduced in \cref{chap:theory} and supports hyperparameter optimization using multiple restarts to avoid bad local optimas. 

\subsection{Choice of Kernel}
The kernel was selected after trial and error as the sum of a single \acrshort{rbf} kernel and a White kernel.

\begin{equation}\label{eq:direct_gp_kernel}
    k(\boldsymbol{\eta}_i, \boldsymbol{\eta}_j) = \sigma_0 k_0(\boldsymbol{\eta}_i, \boldsymbol{\eta}_j) + \sigma_2 \delta(i, j)
\end{equation}

\begin{description}
    \item[Long term trend $k_0$] is a \acrshort{rbf} kernel intended to cover the long-term behavior of the trajectories, i.e., a smooth component describing the overall trend of the trajectories. The lengthscales of this kernel are assumed to be large.
    \item[Independent noise $\delta(i, j)$] is a white kernel intended to model any remaining error as \acrshort{iid}.
\end{description} 

A wide range of other kernels were attempted, such as the Rational Quadratic and Matern class. None of these kernels performed consistently better than \cref{eq:direct_gp_kernel}. A sum of several \acrshort{rbf} kernels gave similar results, but were dropped after signs of overfitting. The current kernel was preferred due to its simplicity. 






%For the \acrshort{svgp} implementation, the Python library \textit{GPFlow} \cite{GPflow2017} built on the well-known \textit{tensorflow} \cite{tensorflow2015-whitepaper} library was used. GPFlow has several implementations of approximate \acrshort{gp}s, including the \acrshort{svgp}. For optimization, the stochastic gradient descent optimizer \textit{ADAM} should be familiar for anyone working with Neural Networks. This optimizer was selected to optimize both the inducing variables as well as the hyperparameters through \textit{mini batching}. Mini Batching is an optimization technique used in combination with stochastic optimization, where a random subset of the training set is used in each iteration instead of the full dataset. This way, big datasets can be used to train the model.\todo[]{This is from memory, find a source}


\section{Limitations}

\subsection{Branching Trajectories}
This approach works well for unimodal trajectory distribution, where there are only minor variations between the trajectories used for training. However, this approach can only express unimodal trajectory distributions due to the Gaussian assumption. In the case of branching trajectories, this method therefore fails as a single Gaussian distribution is attempting to describe several modes at once. The result is a prediction mean somewhere inbetween the branching trajectories and with large uncertainty. 

\subsection{Independent Outputs}
The current formulation assumes independent output dimensions.As a result, the model is unable to express any covariance between the two. In practice, this assumption turns out to be unrealistic as limits the uncertianty to fixed directions only. A better parametrization would perhaps be assume independent lateral and longitudinal components instead, similar to \cite{gp_ais_trajectory}. Decomposing into course and velocity would allow the model more fine-grained control to express uncertianty only in specific directions. Another approach could be to relax the assumption of independent output dimension, though it is unclear how this would be achieved in practice. 


