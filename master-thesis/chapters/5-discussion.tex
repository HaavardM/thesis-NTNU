\chapter{Discussion}\label{chap:discussion}


\section{Independent Outputs}

The formulation of vector-valued \acrshort{gp}s in this thesis assumes independent output dimensions. This choice was made to reduce some of the complexity, as \acrshort{gp}s with dependent outputs complicates the derivations, and there is not a lot of literature on how to select good kernels when the dimensions are dependent. As a result, the \acrshort{gp}s used in this thesis are unable to express any covariance between the outputs. 

This assumption turns out to be a limiting factor for the direct approach as the uncertainty is to given directions only. Without the covariance terms, the model cannot express that the uncertainty is along a given path, such as uncertainty caused by varying velocity. A better parametrization would perhaps assume independent lateral and longitudinal components instead, similar to the formulation used by \cite{gp_ais_trajectory}. Such as a decomposition into course and velocity would allow the model more fine-grained control to express uncertainty only in specific directions. Another approach could be to relax the assumption of independent output dimension, though it is unclear how this would be achieved in practice. 

For the dynamical GP-EKF approach, the problem of independent outputs is less problematic. The iterative \acrshort{ekf} procedure allows the model to express covariance in the prediction uncertainty, even if the \acrshort{gp} $\vec{f}$ is limited to independent outputs. However, reparametrization into lateral and longitudinal components could still be beneficial as completely independent North and East dimensions are unrealistic.  

\section{Shared Kernel}
A similar simplifying assumption is the shared kernel between each output dimension. This choice was also made to reduce complexity and computational efforts, as optimizing the hyperparameters is an area where this method spends considerable time. Doubling the time spent optimizing was therefore avoided. 

The use of standardization of the training data already allowed the use of one shared kernel, even if there were differences in scale for the training data, which was considered flexible enough.

Whether allowing independent kernels will cause the model to perform better is still an open question. However, it would enable the model to learn separate lengthscale and scale parameters for each output dimension which could be beneficial. 

\section{Clustering}
Clustering of trajectories has not been prioritized in this thesis. Instead, the current implementation of the methods filters the available training data before each prediction, using a set of initial conditions. While this method has worked fine in this thesis, considerable computational improvements can be made from clustering the trajectories offline and select the appropriate cluster when performing predictions. The hyperparameters can then be optimized in advance for each cluster.

\section{A Global set of hyperparameters}
The implementation used in this thesis performs hyperparameter selection for each prediction and does not reuse the parameters. In practice, it might be better to find a set of hyperparameters that works well in several cases and leave the parameters fixed. Alternatively, the model could use those parameters as initial values and only perform minor tweaks before each prediction. 

\section{Branching Trajectories}
This direct \acrshort{gp} approach works well for unimodal trajectory distribution, where there are only minor variations between the trajectories used for training. However, it can only express unimodal trajectory distributions due to the Gaussian assumption. For branching trajectories, the method fails as a single Gaussian distribution attempts to describe several modes at once. The result is a prediction mean somewhere in between the branching trajectories and with large uncertainty. There do exist possible extensions to this method, which could, in theory, allow the model to express multimodal uncertainty. Mixtures of \acrshort{gp}s, often called \textit{Mixture of Experts} (ME), could be used to describe the uncertainty as a Gaussian mixture model. The experts are local \acrshort{gp}s which are assigned points from a \textit{manager} based purely on the input \cite{rasmussen}. The number of experts could even go towards infinity by using a Dirichlet process as the manager \cite{dirichlet_process_gp}. While intriguing, these methods quickly become analytically intractable and require approximate inference methods such as \textit{Markov Chain Monte Carlo} (MCMC) or \acrshort{vi}. 

The dynamical approach will, in theory, handle branching trajectories better. The vector-field $\vec{f}$ do indeed express multimodal trajectories as seen in \cref{fig:gp_ekf}, and the trajectory distribution can be found numerically through Sequential Monte-Carlo simulations as seen in \cref{fig:gp_particle}. However, the GP-EKF approach cannot express multimodal uncertainty, as the state is assumed to be a unimodal Gaussian distribution. This forces the GP-EKF to follow one mode, and if unlucky, it might end up trapped between two branching trajectories. The \acrshort{pdaf} was, therefore, implemented to attempt to pull the prediction toward a single mode in the cases where several branching trajectories might influence the prediction model. 

\section{PDAF update}
The \acrshort{pdaf} step for GP-EKF was initially proposed to fix an issue where the GP-EKF would predict turns prematurely. The idea was that the \acrshort{pdaf} update would pull the predictions back towards available position measurements if the predictions were far away from available training data. In practice, however, it turned out to be challenging to find a set of parameters for the \acrshort{pdaf} which works well across a wide range of trajectories. It could be tuned to improve specific predictions where the GP-EKF struggles, but the tuning would negatively affect other predictions. The results from statistical testing in \cref{chap:stat_testing} does not indicate any benefit of using the \acrshort{pdaf} update. The results may perhaps be improved with more tuning efforts, though this is itself a limitation of the \acrshort{pdaf} update. Both the direct \acrshort{gp} implementation and the GP-EKF are purely data-driven, and all parameters are estimated from available data. The \acrshort{pdaf} parameters, therefore, severely limits the flexibility of the model. 

\subsection{Numerical Gradients}
One concern with the GP-EKF is the need for accurate gradients to use for training. Compared to similar approaches in \cite{vehicle_gp_prediction,pedestrian}, the sampling interval of \acrshort{ais} is in the range of several minutes. The finite difference approach for calculating the gradients should, in theory, be a poor choice for curved trajectories, as it would only be able to capture the average velocity over the large sampling interval and be unable to represent the curvature of the trajectory. Several examples where the numerical gradients caused premature turns were found during testing. The gradients were calculated using points before and after a turn, effectively predicting an unrealistic shortcut.  

Using the reported \acrshort{cog} and \acrshort{sog} was therefore expected to perform better. However, the statistical results support the opposite, as the finite difference approach had lower trajectory errors and spread than the \acrshort{cog}/\acrshort{sog} approach. 

The reason why the \acrshort{cog} and \acrshort{sog} performs worse than the finite difference approach is currently an open question. One possible explanation is that even on curved trajectories, the vessels spend most of the time moving in straight lines, where the average velocity is an acceptable estimate.

During development, it was also found that the \acrshort{sog} was a poor estimate of the actual velocity of the vessel. This is because the path was, in many cases, correct while the timing was off. However, the statistical testing does not support this explanation, as the path error is better when using the finite difference approach for long-term predictions, as seen in \cref{table:stats_curved_path_err}.

A possible extension to this work could be to attempt a hybrid approach, where the numerical gradients are combined with the \acrshort{cog} and \acrshort{sog} from \acrshort{ais} to get good average performance while still avoiding premature turns. 


\section{Sparse Variational Gaussian Process}
The \acrshort{svgp} implementation of the direct \acrshort{gp} approach was attempted without success. Even after the training loss started to converge toward an optimum after several hours, the result was unpredictable and unrealistic trajectories. While a few different kernel choices were attempted, the long training time made it inconvenient to work with. Therefore, the efforts were focused elsewhere, as the other methods in this thesis gave better results while also being simpler. 

The \acrshort{svgp} is, however, still included in this thesis to emphasize that there are ways to deal with the high computational complexity of \acrshort{gp}s. While it failed for the problem formulation used in this thesis, the failure is believed to be due to an ill-posed problem rather than an issue with the \acrshort{svgp} itself. The attempt to use \acrshort{svgp} in this thesis tried to do too much at once.