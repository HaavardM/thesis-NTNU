\chapter{Discussion}\label{chap:discussion}

The results from \cref{chap:stat_testing} showcase the \acrshort{gp} framework's ability to predict a vessel's trajectory using historical \acrshort{ais} data. In this section, the results will be discussed further together with theoretical considerations.

\section{A single, global kernel}
Finding a good kernel that works well across a wide range of scenarios can be quite challenging. The choice of kernel forms a prior belief about the underlying function and has a significant impact on the type of function that the \acrshort{gp} will learn. 

In this thesis, the kernels are selected by assuming smooth vessel trajectories and are reasonable for vessels in transit as they tend to change course slowly. 

More specialized vessels, such as local ferries in the \acrshort{ais} dataset, require more complicated kernels to perform well. These vessels move in more distinct patterns and are harder to predict due to the vessel's behavior when docking for short periods before moving back the same way they came. 

The Direct \acrshort{gp} approach performs well with more complicated kernels, and the added flexibility does not significantly impact the consistency of the performance. Overfitting is still a concern, but during the development of the Direct \acrshort{gp} approach, the more flexible kernels were found to yield higher accuracy in most scenarios. However, this thesis does not focus too much on kernel design, and no statistical testing is performed when using different kernels.

The GP-EKF is more sensitive to the choice of kernel, and as already discussed in \cref{chap:gp_ekf}, it may lead to instabilities if the kernel and its hyperparameters are not chosen carefully.  

\section{Sensitivity to parameters}
The results for the GP-EKF in \cref{chap:stat_testing} heavily depend on the choice of parameters. The basic GP-EKF configuration depends on the initial uncertainty $\boldsymbol{P}_0$, and as it is a pure prediction method, it has a significant impact on the resulting trajectory uncertainty. The \acrshort{sl} update and the \acrshort{pdaf} update also heavily depend on the choice of their respective parameters. Finding a suitable procedure for selecting good parameters for both of these methods is still an open research question. 

The problem with both of the update methods is the lack of an appropriate interpretation of the parameters. For the \acrshort{sl} approach, it is unknown what the measurement noise $\boldsymbol{R}$ should be, and it is currently tuned by trial and error. The \acrshort{pdaf} update suffers from a similar problem, as the clutter rate $\lambda$ and the detection probability $p_D$ currently has no good interpretation when used for trajectory prediction. 

The \acrshort{gp}s also depend on the result of hyperparameter optimization. The \acrshort{ml} approach from \cref{sec:gp_mle} works well in most cases, but there are several issues with unrealistic hyperparameters causing trouble. It is especially a problem for GP-EKF, as the best parameters for the motion model $\vec{f}$ does not necessarily yield the best trajectory prediction, and the Jacobian is sensitive to short length scales. Therefore, it is vital to keep the numerous approximations used by GP-EKF in mind, as they add artificial limitations to the motion model $\vec{f}$.

\section{Incorporating position in GP-EKF predictions}
Considering the statistical results for the \acrshort{pdaf} and \acrshort{sl} updates in \cref{fig:stats_gp_ekf_with_or_without_update}, there is little evidence to support that these methods for incorporating position data actually have any benefit on average. The problem is that both these methods make assumptions about the target vessel's predicted position, and the historical \acrshort{ais} samples originate from the same underlying trajectory. In practice, the \acrshort{ais} data contains samples from a wide range of different overlapping trajectories. The \acrshort{pdaf} is, in theory, supposed to reduce the effect of this issue with the spread-of-innovation term. However, any effort put into finding a good set of parameters has been unsuccessful. 

The question becomes whether there is an actual need for these methods or if the basic GP-EKF performs well enough on its own. Practical experience with these methods suggests that using only the open-loop GP-EKF prediction often results in underconfident estimates, especially if the true trajectory distribution is multimodal. For example, in branching trajectories, the basic GP-EKF might get stuck between two trajectories, causing significant uncertainty and a trajectory estimate that follows neither of the branching trajectories. Incorporating position data allows the model to correct for such poor estimates and yields estimates that are more consistent with the training data. However, the downside is that it might end up following the wrong trajectory, as can be seen in \cref{fig:app_branching_gp_ekf_update}.

There is also the argument of improving the stability of the predictions. The basic GP-EKF is sensitive to outlier samples, which might significantly impact the predicted trajectory distribution. An update step can correct errors induced by such outlier samples and improve the robustness of the predictions. 

Neither the \acrshort{sl} update nor the \acrshort{pdaf} update is a satisfactory solution to this problem, as both tend to result in overconfident estimates.  



\section{Independent Outputs}
The formulation of vector-valued \acrshort{gp}s in this thesis assumes independent output dimensions. This choice is intended to reduce some of the complexity, as \acrshort{gp}s with dependent outputs complicate the derivations, and there is less literature on how to select good kernels when the dimensions are dependent. As a result, the \acrshort{gp}s used in this thesis are unable to express any covariance between the outputs. 

This assumption is a limiting factor for the direct \acrshort{gp} approach as the uncertainty is constrained to specific directions. Without the covariance terms, the model cannot express that the uncertainty is along a given path, such as uncertainty caused by differences in velocity between training samples. A better parametrization may be to instead assume independent lateral and longitudinal components, similar to the formulation used by \cite{gp_ais_trajectory}. Such a decomposition into course and velocity would allow the model more fine-grained control to express uncertainty only in specific directions. Another approach could be to relax the assumption of independent output dimension, though it is unclear how this would be achieved in practice. 

For the dynamical GP-EKF approach, the problem of independent outputs is less problematic. The iterative \acrshort{ekf} procedure and propagation of uncertainty allow the model to express covariance in the prediction uncertainty, even if the \acrshort{gp} $\vec{f}$ is limited to independent outputs. However, reparametrization into lateral and longitudinal components could still be beneficial

\section{Shared Kernel}
A similar simplifying assumption is the shared kernel between each output dimension. This choice was also made to reduce complexity and computational efforts, as optimizing the hyperparameters is where this method spends considerable time. Doubling the time spent optimizing was therefore avoided. 

Standardization of the training data allows the use of one shared kernel, even if there are differences in scale for the training data, which was considered flexible enough in this thesis.

Whether allowing independent kernels will cause the model to perform better is still an open question. However, it would enable the model to learn separate lengthscale and scale parameters for each output dimension which could be beneficial. 



\section{Clustering}
Clustering of trajectories has not been prioritized in this thesis. Instead, the current implementation of the methods filters the available training data before each prediction, using a set of initial conditions. While this method has worked fine in this thesis, considerable computational improvements can be made from clustering the trajectories offline and select the appropriate cluster when performing predictions. The hyperparameters can then be optimized in advance for each cluster. Combining this work with trajectory clustering methods such as TRACLUS \cite{traclus} may be a good way to tackle some of the computational challenges of the methods proposed in this thesis.



\section{Branching Trajectories}
This direct \acrshort{gp} approach works well for unimodal trajectory distribution, where there are only minor variations between the trajectories used for training. However, it can only express unimodal trajectory distributions due to the Gaussian assumption. Moreover, the method fails on branching trajectories as a single Gaussian distribution attempts to describe several modes at once. The result is a prediction mean somewhere in between the branching trajectories and with large uncertainty. 

The motion model used by GP-EKF will, in theory, handle branching trajectories better. The vector-field $\vec{f}$ do indeed express multimodal trajectories as seen in \cref{fig:gp_ekf}, and the trajectory distribution can, for instance, be found numerically through Sequential Monte-Carlo simulations as seen in \cref{fig:gp_particle}. However, the GP-EKF approach cannot express multimodal uncertainty, as the state is assumed to be a unimodal Gaussian distribution. This assumption forces the GP-EKF to express the distribution as unimodal.  

These limitations motivate further research into Sequential Monte-Carlo methods using the same \acrshort{gp} based motion model proposed in \cref{chap:gp_ekf} for the GP-EKF.  

\section{Numerical Gradients}
One concern with the GP-EKF is the need for accurate gradients to use for training. Compared to similar approaches in \cite{vehicle_gp_prediction,pedestrian}, the sampling interval of \acrshort{ais} is in the range of several minutes. The finite difference approach for calculating the gradients should, in theory, be a poor choice for curved trajectories, as it would only be able to capture the average velocity over the large sampling interval and be unable to represent the curvature of the trajectory. Several examples where the numerical gradients caused premature turns were found during testing. The gradients were calculated using points before and after a turn, effectively predicting an unrealistic shortcut.  Using the reported \acrshort{cog} and \acrshort{sog} was therefore expected to perform better. 
However, when comparing the two possible sources in \cref{fig:stats_gp_ekf_fd_vs_cog}, the finite difference approach turns out to perform better. 

A possible extension to this work could be to attempt a hybrid approach, where the numerical gradients are combined with the \acrshort{cog} and \acrshort{sog} from \acrshort{ais} to get good average performance while still avoiding premature turns. 

\section{GP-EKF time dependency}
The time component of the GP-EKF's motion model was initially added to handle trajectories with sharp turns better. For example, if $\vec{f}$ only relied on position, the \acrshort{gp} would have no way of expressing that the speed vector at position $\boldsymbol{x}$ might change over time. The time component was later found to explain some of the variability in the velocity, yielding better uncertainty estimates.

In practice, the \acrshort{ml} approach for hyperparameter optimization will, in many cases, yield very large lengthscales\footnote{In fact, the time lengthscale tends to reach a maximum value during optimization} for the time-components of the kernel, effectively disabling the time dependency. The time component is therefore only really used when the dataset contains a lot of time-dependent noise.

However, the added dimension and corresponding hyperparameter also increase the risk of overfitting \cite{rasmussen}, and whether or not to include the time dependency becomes a tradeoff between complexity and performance. For highly smooth trajectories, it is unlikely to be necessary. 

\subsection{Numerical issues}
As with any other data-driven method, the results of using \acrshort{gp}s heavily depend on the quality of the data. Outlier samples, such as a vessel suddenly making a u-turn, can cause issues as these samples deviate significantly from the assumed Gaussian distribution.

The problem is especially problematic for the basic GP-EKF implementation. For example, for \acrshort{gp} input values close to one such outlier, the resulting function has a high degree of non-linearity, which is problematic considering the Taylor approximation. However, the update steps can, to some extend, alleviate the problem. 

While it certainly affects the Direct \acrshort{gp} as well, the problem is less severe as it directly interpolates the position data.  

\section{Computational Complexity}
A key concern with using \acrshort{gp}s is the computational complexity required to invert the covariance matrix and solve large systems of linear equations. Indeed, the exact formulation of \acrshort{gp}s is constrained to small datasets only, and the computations quickly become infeasible as the number of samples increases due to the cubic complexity. Practical experience with \acrshort{gp}s indicates that working with several thousand samples works just fine, as long as the hyperparameters are kept fixed. However, it is more problematic for hyperparameter optimization, as the marginal likelihood from \cref{eq:gp_log_marginal_likelihood} requires the inversion of the entire covariance matrix at each iteration, which tends to get tediously slow when using more than $N=1000$ samples. 

The hyperparameter optimization turns out to be the main bottleneck with the methods proposed in this thesis. This issue motivates the need for more clever ways to select hyperparameters. For example, a set of global hyperparameters could be used for real-time applications instead of optimizing for each scenario. Furthermore, as there are likely some variations between different vessels and scenarios, clustering could be used as an alternative to preparing a fixed number of hyperparameter to be used in real-time.

Even in an offline setting, hyperparameter optimization quickly becomes infeasible for large datasets. In a real-world application, it might be necessary to use approximate methods instead. Variational formulations of the \acrshort{gp}, such as the \acrshort{svgp}, be good solutions. In combination with stochastic optimization and mini-batching, these approximations can utilize large datasets during training. However, these approximate methods also add a substantial amount of complexity, causing the\acrshort{gp} to lose its advantage over other methods such as neural networks.



%There do exist possible extensions to this method, which could, in theory, allow the model to express multimodal uncertainty. Mixtures of \acrshort{gp}s, often called \textit{Mixture of Experts} (ME), could be used to describe the uncertainty as a Gaussian mixture model. The experts are local \acrshort{gp}s which are assigned points from a \textit{manager} based purely on the input \cite{rasmussen}. The number of experts could even go towards infinity by using a Dirichlet process as the manager \cite{dirichlet_process_gp}. While intriguing, these methods quickly become analytically intractable and require approximate inference methods such as \textit{Markov Chain Monte Carlo} (MCMC) or \acrshort{vi}. 