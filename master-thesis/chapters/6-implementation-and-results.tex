\chapter{Statistical Testing}\label{chap:stat_testing}
Inspired by \cite{hexeberg}, the performance of the proposed models will be compared on straight-line and curved trajectories independently.

\section{Method of testing}
\subsection{Trajectory Error}
The trajectory error is found by comparing the predicted position with the ground truth. As the predicted trajectory is simulated in discrete time, the points with the closest timestamps are used for comparison. With $\Delta T = 10\text{ seconds}$, the maximum error in time is $\frac{\Delta T}{2} = 5 \text{ seconds}$, which is considered to be acceptable considering the time-horizon of between $15$ and $30$ minutes.
\subsection{Path Error}
The path error is defined as the closest point in the predicted trajectory to each point in the ground truth, under the constraint that the corresponding predicted timestamps must be monotonically increasing. In other words, the path cannot move backward in time. Linear interpolation is used to get the path error at fixed timestamps to simplify the comparison.

\subsection{Normalized Estimation Error Squared}
For the uncertainty estimates to provide any value, the predictions must be consistent. In this context, the term consistency is borrowed from the term \textit{filter consistency} used when tuning Kalman filters \cite{sensorfusjon}. The idea is that prediction errors, on average, should scale with the state covariance. In other words, the model should not place much confidence in a prediction that is wrong while being highly confident when a prediction is correct. Consistency can also be interpreted using a frequentistic interpretation of probability, where after many predictions, the state uncertainty should reflect the actual error rate.

The \textit{\acrfull{nees}} is a metric that can be used to quantify consistency and is given by
\begin{equation}
    \text{NEES} = (\boldsymbol{x} - \hat{\boldsymbol{x}})^\intercal \boldsymbol{P}^{-1} (\boldsymbol{x} - \hat{\boldsymbol{x}})
\end{equation}

Assuming the prediction error follows a Gaussian distribution, the \acrshort{nees} follows a Chi-Squared distribution which can be used to form a confidence interval. Comparing the prediction errors with this confidence interval can then be used to get a sense of whether the estimated state uncertainty is consistent with the actual error rate.

\subsection{Interpolation}
Linear interpolation is used to compare the metrics at fixed timestamps between samples. The error for short trajectories is not extrapolated.

\subsection{Baseline - Constant Velocity Model}
As a basis of comparison, the \textit{\acrfull{cvm}} method is used as a baseline. The model uses the initial \acrshort{cog} and \acrshort{sog} to predict a straight line, where the vessel is assumed to keep a constant velocity and heading.

\subsection{Sanitizing the dataset for a given test trajectory}
Due to the way trajectories are generated from the \acrshort{ais} dataset, there will be large overlaps between trajectories. Therefore, dividing the trajectories into a train and test is insufficient, as parts of a test trajectory might also exist in the training set. Instead, the entire dataset is available for training, but all trajectories with identical MMSI and date as the test trajectory are removed. The date requirement ensures that trajectories for the same vessel can be used for training on any other day.

\subsection{Straigth-line trajectories}
The GP-EKF is first compared to the \acrshort{cvm} on simple straight-line trajectories. The GP-EKF should ideally perform identically or better than the \acrshort{cvm}.
The statistics are based on $350$ randomly sampled trajectories without replacement that satisfies the following requirements:
\begin{enumerate}
    \item The sum of subsequent changes in \acrshort{cog} must be less than $30$ degrees, i.e. $\sum_i |(\mathcal{X}_{t+1} - \mathcal{X}_t)| \leq 30^\circ$. This requirement ensures a straight-line trajectory.
    \item There must be sufficient data available for training in the neighborhood around the initial starting point, with similar initial heading and speed. After sanitizing the dataset and removing irrelevant trajectories, at least $3$ trajectories need to be available for training.
    \item The overall duration of the trajectories must be between $15$ and $30$ minutes.
\end{enumerate}




\subsection{Curved Trajectories}
The GP-EKF is then compared to the \acrshort{cvm} on curved trajectories. The GP-EKF should ideally perform drastically better than the \acrshort{cvm}.
The statistics are based on $350$ randomly sampled trajectories that satisfy the following requirements:
\begin{enumerate}
    \item The sum of subsequent changes in \acrshort{cog} must be greater than $40$ degrees, i.e. $\sum_i |(\mathcal{X}_{t+1} - \mathcal{X}_t)| \geq 40^\circ$. This requirement ensures a straight-line trajectory.
    \item There must be sufficient data available for training in the neighborhood around the initial starting point, with similar initial heading and speed. After sanitizing the dataset and removing irrelevant trajectories, at least $3$ trajectories need to be available for training.
    \item The overall duration of the trajectories must be between $15$ and $30$ minutes.
\end{enumerate}


\section{Implementation}
The implementation details for each of the methods are described in this section.

\subsection{GP-EKF}
The GP-EKF is tested with and without the \acrshort{pdaf} update step, as well as using both finite difference and the \acrshort{cog}/\acrshort{sog} from the \acrshort{ais} dataset, resulting in $4$ different instances.

The GP-EKF required more flexibility during development. Due to the need for calculating the gradient $\frac{\delta \vec{f}}{\delta \boldsymbol{x}}$, it became impractical to use existing \acrshort{gp} implementations. Due to the simple implementation of \cref{alg:gp_prediction}, it was easier to implement it from the ground up.
The GP-EKF was therefore implemented directly in Python using only \textit{scipy}\cite{scipy} and \textit{numpy}\cite{numpy} to speed up linear algebra routines. The Cholesky decomposition in \cref{alg:gp_prediction} was computed using \texttt{scipy.linalg.cho\_factor}, which calls a highly optimized LAPACK routine. Similarily, \texttt{scipy.linalg.solve\_\-trianglular} was used to solve the lower triangular system of equations by forward substitution. The implementation of GP-EKF and the \acrshort{pdaf} update was then straightforward using \cref{alg:gp_ekf_prediction} and \cref{alg:gp_ekf_pdaf} from \cref{chap:gp_ekf}

The hyperparameters were tuned using the \acrshort{gp} implementation in the popular \textit{scikit-learn} \cite{scikit-learn} Python package. $10$ random restarts were used during optimization to reduce the risk of bad local minima. Due to the wide variety of different scenarios on the training set, the hyperparameters are optimized for each simulation iteration. This way, the robustness of the optimization is indirectly tested, as instances of bad local optima are included in the results.


The remaining parameters not found through \acrshort{ml} were tuned through trial and error on a few different trajectories.
The initial state uncertainty is set to $\boldsymbol{P}_0 = 500^2 \cdot \boldsymbol{I}$, which was found to work well during development. The \acrshort{pdaf} parameters are available in \cref{table:stats_pdaf_params}.

\begin{table}[h]
    \centering
    \begin{tabular}{lll}
        \textit{\textbf{Parameter}} &                  & \textit{\textbf{Value}}     \\ \hline
        Measurement noise           & $\boldsymbol{R}$ & $50^2 \cdot \boldsymbol{I}$ \\
        Detection Probability       & $p_D$            & $0.8$                       \\
        Clutter Rate                & $\lambda$        & $2 \cdot 10^{-3}$           \\
        Gate Size                   & $g$              & $2$
    \end{tabular}
    \caption{Parameters used for \acrshort{pdaf} update}
    \label{table:stats_pdaf_params}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{lll}
        \textit{\textbf{Parameter}} &                  & \textit{\textbf{Value}}       \\ \hline
        Noise                       & $\boldsymbol{R}$ & $2000^2 \cdot \boldsymbol{I}$ \\
        Search Radius               & $500$
    \end{tabular}
    \caption{Parameters used for the SL update}
    \label{table:stats_sl_params}
\end{table}

\subsection{Direct GP}
The excact \acrshort{gp} formulation was implemented using the \texttt{GaussianProcessRegressor} from the popular Python library, \textit{sciki-learn} \cite{scikit-learn}. The library support all kernels introduced in \cref{chap:theory} and supports hyperparameter optimization using multiple restarts to avoid bad local optimas.


%For the \acrshort{svgp} implementation, the Python library \textit{GPFlow} \cite{GPflow2017} built on the well-known \textit{tensorflow} \cite{tensorflow2015-whitepaper} library was used. GPFlow has several implementations of approximate \acrshort{gp}s, including the \acrshort{svgp}. For optimization, the stochastic gradient descent optimizer \textit{ADAM} should be familiar for anyone working with Neural Networks. This optimizer was selected to optimize both the inducing variables as well as the hyperparameters through \textit{mini batching}. Mini Batching is an optimization technique used in combination with stochastic optimization, where a random subset of the training set is used in each iteration instead of the full dataset. This way, big datasets can be used to train the model.\todo[]{This is from memory, find a source} 

Only the exact formulation of the direct \acrshort{gp} approach is included in the following testing for two reasons:
\begin{enumerate}
    \item All methods can use the same representative trajectories for training. As a result, it becomes easier to compare the different methods as the performance is not influenced by how the training data is selected.
    \item The \acrshort{svgp} turned out to be very complicated to make work. Even after several hours of training, the model performed rather poorly, with unrealistic trajectory estimates. It was decided that it was not worth the additional effort of performing statistical testing.
\end{enumerate}

\section{Results}
The results for straight-line and curved trajectories are available in \cref{table:stats_straight_line_error} and \cref{table:stats_curved_error} respectively. \todo[]{add examples in appendix?}.

The GP-EKF using finite-difference for training data is considered to be the basic configuration of the GP-EKF, and will be used as a baseline when comparing the different variants of the GP-EKF. Unless explicitly stated otherwise, it is therefore same to assume the GP-EKF is using finite-difference and no update step.

As a base of comparison, the GP-EKF and Direct GP are both compared to the \acrshort{cvm} model on both straight-line and curved trajectories in \cref{fig:stats_both_vs_cvm}. On straight-line trajectories, both approaches performs worse than the \acrshort{cvm}. On curved trajectories the \acrshort{cvm} struggles as would be expected, whereas both the GP-EKF and Direct GP approaches perform significantly better with both lower median error and spread. The Direct GP approach performs slightly better than the GP-EKF, with overall lower error quartiles.

The GP-EKF in its basic configuration will be used as a reference when exploring the outcome of different design-choices.

\subsection{GP-EKF: Finite Difference vs. COG/SOG from AIS}

\begin{figure}[h]
    \centering
    \makebox[\textwidth][c]{
        \begin{subfigure}{0.65\textwidth}
            \includegraphics{figures/straight_line_stats/gp_cog_vs_fd.pdf}
            \caption{Straight-Line Trajectory}
        \end{subfigure}
        \begin{subfigure}{0.65\textwidth}
            \includegraphics{figures/curved_line_stats/gp_cog_vs_fd.pdf}
            \caption{Curved Trajectory}
        \end{subfigure}
    }
    \caption{GP-EKF using finite differences and the \acrshort{cog} and \acrshort{sog} from the AIS dataset on $350$ trajectories. The finite differences approach performs consistently better, with lower median error and spread.}
    \label{fig:stats_gp_ekf_fd_vs_cog}
\end{figure}


A key design choice for the GP-EKF is which data source to use for training. The model can either be trained using the \acrshort{cog} and \acrshort{sog} values contained in the \acrshort{ais} samples or by calculating numerical derivatives of the position through a finite-difference approach. \cref{fig:stats_gp_ekf_fd_vs_cog} compares the difference side-by-side and the results are remarkably similar.

\subsection{GP-EKF: Incorporating positions data}
The effect of both the \acrshort{pdaf} and the \acrshort{sl} update procedures are compared to the basic GP-EKF configuration in \cref{fig:stats_gp_ekf_with_or_without_update}. \todo[]{Add straightline comment}. For the curved trajectories, the results are remarkably similar. On closer inspection, there appears to be a slight advantage to using the \acrshort{pdaf} update step, but the improvement is within the margin of error so it is not possible to draw any conclusion. By looking closely, there also appear to be a slight benefit of using the \acrshort{sl} update, though the improvement is marginal.




\subsubsection{The modified update}





\begin{table}
    \begin{subtable}{\textwidth}
        \makebox[\textwidth][c]{
            \begin{tabular}{lllrrrrr}
                \toprule
                        &                & Time [Minutes]    & 5   & 10  & 15  & 20   & 25   \\
                Summary & Method         & Training Source   &     &     &     &      &      \\
                \midrule
                Mean    & CVM            & COG/SOG from AIS  & 235 & 473 & 756 & 1105 & 1741 \\
                        & Direct GP      & Position          & 390 & 560 & 800 & 1146 & 1693 \\
                        & GP-EKF         & COG/SOG from AIS  & 345 & 654 & 928 & 1223 & 1875 \\
                        &                & Finite Difference & 310 & 570 & 843 & 1099 & 1384 \\
                        & GP-EKF w/ PDAF & COG/SOG from AIS  & 351 & 623 & 884 & 1108 & 1507 \\
                        &                & Finite Difference & 291 & 522 & 786 & 1075 & 1159 \\
                \midrule
                Median  & CVM            & COG/SOG from AIS  & 124 & 253 & 448 & 682  & 1291 \\

                        & Direct GP      & Position          & 237 & 400 & 552 & 843  & 1274 \\
                        & GP-EKF         & COG/SOG from AIS  & 284 & 549 & 745 & 984  & 1174 \\
                        &                & Finite Difference & 256 & 490 & 677 & 867  & 834  \\
                        & GP-EKF w/ PDAF & COG/SOG from AIS  & 282 & 505 & 693 & 976  & 1144 \\
                        &                & Finite Difference & 243 & 413 & 651 & 849  & 839  \\
                \bottomrule
            \end{tabular}
        }
        \caption{Trajectory errors in meters}
        \label{table:stats_straight_traj_err}
        \vspace*{0.5cm}
    \end{subtable}
    \begin{subtable}{\textwidth}
        \makebox[\textwidth][c]{
            \begin{tabular}{lllrrrrr}
                \toprule
                        &                & Time [Minutes]    & 5   & 10  & 15  & 20  & 25   \\
                Summary & Method         & Training Source   &     &     &     &     &      \\
                \midrule
                Mean    & CVM            & COG/SOG from AIS  & 55  & 178 & 364 & 559 & 1195 \\
                        & Direct GP      & Position          & 195 & 236 & 364 & 602 & 1390 \\
                        & GP-EKF         & COG/SOG from AIS  & 133 & 240 & 343 & 394 & 636  \\
                        &                & Finite Difference & 143 & 266 & 395 & 500 & 630  \\
                        & GP-EKF w/ PDAF & COG/SOG from AIS  & 128 & 234 & 341 & 352 & 513  \\
                        &                & Finite Difference & 131 & 224 & 344 & 503 & 656  \\
                \midrule
                Median  & CVM            & COG/SOG from AIS  & 29  & 96  & 222 & 358 & 711  \\
                        & Direct GP      & Position          & 77  & 116 & 174 & 279 & 907  \\
                        & GP-EKF         & COG/SOG from AIS  & 86  & 166 & 232 & 278 & 665  \\
                        &                & Finite Difference & 90  & 171 & 258 & 375 & 507  \\
                        & GP-EKF w/ PDAF & COG/SOG from AIS  & 79  & 144 & 197 & 233 & 388  \\
                        &                & Finite Difference & 79  & 135 & 198 & 301 & 354  \\
                \bottomrule
            \end{tabular}

        }
        \caption{Path error in meters}
        \label{table:stats_straight_path_err}
    \end{subtable}
    \caption{Error summary for $350$ straight-line trajectories. Mean and median summary statistics are calculated for the trajectory and path error at fixed timestamps. Linear interpolation is used between samples. Errors for short trajectories are not extrapolated, and therefore not included in the $20$ and $25$ minute bins.}
    \label{table:stats_straight_line_error}
\end{table}

\begin{table}
    \begin{subtable}{\textwidth}
        \makebox[\textwidth][c]{
            \begin{tabular}{lllrrrrr}
                \toprule
                        &                & Time [Minutes]    & 5   & 10   & 15   & 20   & 25   \\
                Summary & Method         & Training Source   &     &      &      &      &      \\
                \midrule
                Mean    & CVM            & COG/SOG from AIS  & 440 & 1071 & 1898 & 2425 & 3313 \\
                        & Direct GP      & Position          & 366 & 522  & 771  & 993  & 1298 \\
                        & GP-EKF         & COG/SOG from AIS  & 352 & 674  & 950  & 1327 & 1856 \\
                        &                & Finite Difference & 329 & 575  & 823  & 1081 & 1388 \\
                        & GP-EKF w/ PDAF & COG/SOG from AIS  & 478 & 904  & 1255 & 1473 & 1824 \\
                        &                & Finite Difference & 371 & 610  & 835  & 1056 & 1220 \\
                        & GP-EKF w/ SL   & COG/SOG from AIS  & 340 & 636  & 874  & 1194 & 1592 \\
                        &                & Finite Difference & 324 & 555  & 795  & 1030 & 1312 \\
                \hline
                Median  & CVM            & COG/SOG from AIS  & 167 & 452  & 1009 & 1561 & 2359 \\
                        & Direct GP      & Position          & 229 & 364  & 550  & 705  & 942  \\
                        & GP-EKF         & COG/SOG from AIS  & 257 & 483  & 724  & 1078 & 1505 \\
                        &                & Finite Difference & 255 & 463  & 635  & 844  & 1120 \\
                        & GP-EKF w/ PDAF & COG/SOG from AIS  & 368 & 654  & 923  & 1128 & 1327 \\
                        &                & Finite Difference & 315 & 493  & 651  & 704  & 854  \\
                        & GP-EKF w/ SL   & COG/SOG from AIS  & 251 & 452  & 646  & 989  & 1357 \\
                        &                & Finite Difference & 256 & 434  & 626  & 789  & 1113 \\
                \bottomrule
            \end{tabular}
        }
        \caption{Trajectory Error in meters}
        \label{table:stats_curved_traj_err}
        \vspace*{0.5cm}
    \end{subtable}
    \begin{subtable}{\textwidth}
        \makebox[\textwidth][c]{
            \begin{tabular}{lllrrrrr}
                \toprule
                        &                & Time [Minutes]    & 5   & 10  & 15   & 20   & 25   \\
                Summary & Method         & Training Source   &     &     &      &      &      \\
                \midrule
                Mean    & CVM            & COG/SOG from AIS  & 204 & 574 & 1072 & 1518 & 2045 \\
                        & Direct GP      & Position          & 175 & 231 & 403  & 596  & 827  \\
                        & GP-EKF         & COG/SOG from AIS  & 143 & 269 & 356  & 484  & 687  \\
                        &                & Finite Difference & 160 & 291 & 412  & 466  & 509  \\
                        & GP-EKF w/ PDAF & COG/SOG from AIS  & 177 & 355 & 587  & 775  & 1218 \\
                        &                & Finite Difference & 155 & 277 & 413  & 515  & 655  \\
                        & GP-EKF w/ SL   & COG/SOG from AIS  & 137 & 240 & 300  & 392  & 522  \\
                        &                & Finite Difference & 151 & 265 & 375  & 437  & 523  \\
                \hline
                Median  & CVM            & COG/SOG from AIS  & 54  & 265 & 715  & 1138 & 1863 \\
                        & Direct GP      & Position          & 81  & 140 & 234  & 336  & 447  \\
                        & GP-EKF         & COG/SOG from AIS  & 99  & 152 & 216  & 295  & 408  \\
                        &                & Finite Difference & 106 & 180 & 323  & 336  & 377  \\
                        & GP-EKF w/ PDAF & COG/SOG from AIS  & 97  & 157 & 290  & 401  & 908  \\
                        &                & Finite Difference & 101 & 182 & 304  & 371  & 402  \\
                        & GP-EKF w/ SL   & COG/SOG from AIS  & 92  & 133 & 195  & 227  & 302  \\
                        &                & Finite Difference & 101 & 173 & 290  & 309  & 395  \\
                \bottomrule
            \end{tabular}
        }
        \caption{Path error in meters}
        \label{table:stats_curved_path_err}
    \end{subtable}
    \caption{Error summary for $350$ curved trajectories. Mean and median summary statistics are calculated for the trajectory and path error at fixed timestamps. Linear interpolation is used between samples. Errors for short trajectories are not extrapolated, and therefore not included in the $20$ and $25$ minute bins.}
    \label{table:stats_curved_error}
\end{table}

\begin{figure}
    \centering
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{figures/straight_line_stats/both_vs_cvm.pdf}

        \caption{Straight-line trajectories.}
        \label{fig:stats_curved_both_vs_cvm}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{figures/curved_line_stats/both_vs_cvm.pdf}
        \caption{Curved trajectories.}
        \label{fig:stats_straight_both_vs_cvm}
    \end{subfigure}
    \caption{GP-EKF and Direct GP compared to a \acrshort{cvm} on both curved and straigth-line trajectories. In both cases, the results are from 350 simulations.}
    \label{fig:stats_both_vs_cvm}
\end{figure}



\begin{figure}[h]
    \centering
    \begin{subfigure}{1\textwidth}
        \includegraphics[width=\textwidth]{figures/straight_line_stats/gp_vs_update.pdf}
        \caption{Straight-Line Trajectories}
    \end{subfigure}
    \begin{subfigure}{1\textwidth}
        \includegraphics[width=\textwidth]{figures/curved_line_stats/gp_vs_update.pdf}
        \caption{Curved Trajectories}
    \end{subfigure}
    \caption{GP-EKF with and without PDAF on curved trajectories for $350$ trajectories. While there are some slight differences, the \acrshort{pdaf} does not appear to have any considerable effect on the trajectory errors.}
    \label{fig:stats_gp_ekf_with_or_without_update}
\end{figure}

\begin{figure}
    \centering
    \makebox[\textwidth][c]{
        \begin{subfigure}{0.65\textwidth}
            \includegraphics{figures/straight_line_stats/nees.pdf}
            \caption{Straight-Line Trajectory}
        \end{subfigure}
        \begin{subfigure}{0.65\textwidth}
            \includegraphics{figures/curved_line_stats/nees.pdf}
            \caption{Curved Trajectory}
        \end{subfigure}
    }
\end{figure}

% On straight-line trajectories, the $25\%$ and $50\%$ quartiles correspond well with the theoretical values for the $\mathcal{X}^2$ distribution for all methods. The \acrshort{pdaf} update step does, however, reduce the predicted uncertainty, leading to increased overconfidence. In addition, the estimated \acrshort{nees} distribution has a much fatter right-tail than the theoretical $\mathcal{X}^2$ distribution, leading to an estimated $75\%$ quartile far outside the theoretical values.

% On curved trajectories, the GP-EKF with and without PDAF is consistently overconfident. The theoretical quartiles are not plotted due to the large y-axis. The direct \acrshort{gp} approach performs significantly better, which is why a separate plot in \cref{fig:stats_curved_nees_direct} is added to compare it to theoretical quartiles.




