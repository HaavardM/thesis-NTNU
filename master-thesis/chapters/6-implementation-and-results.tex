\chapter{Statistical Testing}\label{chap:stat_testing}
Inspired by \cite{hexeberg}, the performance of the proposed models will be compared on straight-line and curved trajectories independently.

\section{Method of testing}
\subsection{Trajectory Error}
The trajectory error is found by comparing the predicted position with the ground truth. As the predicted trajectory is simulated in discrete time, the points with the closest timestamps are used for comparison. With $\Delta T = 10\text{ seconds}$, the maximum error in time is $\frac{\Delta T}{2} = 5 \text{ seconds}$, which is considered to be acceptable considering the time-horizon of between $15$ and $30$ minutes.
\subsection{Path Error}
The path error is defined as the closest point in the predicted trajectory to each point in the ground truth, under the constraint that the corresponding predicted timestamps must be monotonically increasing. In other words, the path cannot move backward in time. Linear interpolation is used to get the path error at fixed timestamps to simplify the comparison.

\subsection{Normalized Estimation Error Squared}
For the uncertainty estimates to provide any value, the predictions must be consistent. In this context, the term consistency is borrowed from the term \textit{filter consistency} used when tuning Kalman filters \cite{sensorfusjon}. The idea is that prediction errors, on average, should scale with the state covariance. In other words, the model should not place much confidence in a prediction that is wrong while being highly confident when a prediction is correct. Consistency can also be interpreted using a frequentistic interpretation of probability, where after many predictions, the state uncertainty should reflect the actual error rate.

The \textit{\acrfull{nees}} is a metric that can be used to quantify consistency and is given by
\begin{equation}
    \text{NEES} = (\boldsymbol{x} - \hat{\boldsymbol{x}})^\intercal \boldsymbol{P}^{-1} (\boldsymbol{x} - \hat{\boldsymbol{x}})
\end{equation}

Assuming the prediction error follows a Gaussian distribution, the \acrshort{nees} follows a Chi-Squared distribution which can be used to form a confidence interval. Comparing the prediction errors with this confidence interval can then be used to get a sense of whether the estimated state uncertainty is consistent with the actual error rate.

\subsection{Interpolation}
Linear interpolation is used to compare the metrics at fixed timestamps between samples. The error for short trajectories is not extrapolated.

\subsection{Baseline - Constant Velocity Model}
As a basis of comparison, the \textit{\acrfull{cvm}} method is used as a baseline. The model uses the initial \acrshort{cog} and \acrshort{sog} to predict a straight line, where the vessel is assumed to keep a constant velocity and heading.

\subsection{Sanitizing the dataset for a given test trajectory}
Due to the way trajectories are generated from the \acrshort{ais} dataset, there will be large overlaps between trajectories. Therefore, dividing the trajectories into a train and test is insufficient, as parts of a test trajectory might also exist in the training set. Instead, the entire dataset is available for training, but all trajectories with identical MMSI and date as the test trajectory are removed. The date requirement ensures that trajectories for the same vessel can be used for training on any other day.

\subsection{Straigth-line trajectories}
The GP-EKF is first compared to the \acrshort{cvm} on simple straight-line trajectories. The GP-EKF should ideally perform identically or better than the \acrshort{cvm}.
The statistics are based on $350$ randomly sampled trajectories without replacement that satisfies the following requirements:
\begin{enumerate}
    \item The sum of subsequent changes in \acrshort{cog} must be less than $30$ degrees, i.e. $\sum_i |(\mathcal{X}_{t+1} - \mathcal{X}_t)| \leq 30^\circ$. This requirement ensures a straight-line trajectory.
    \item There must be sufficient data available for training in the neighborhood around the initial starting point, with similar initial heading and speed. After sanitizing the dataset and removing irrelevant trajectories, at least $3$ trajectories need to be available for training.
    \item The overall duration of the trajectories must be between $15$ and $30$ minutes.
\end{enumerate}




\subsection{Curved Trajectories}
The GP-EKF is then compared to the \acrshort{cvm} on curved trajectories. The GP-EKF should ideally perform drastically better than the \acrshort{cvm}.
The statistics are based on $350$ randomly sampled trajectories that satisfy the following requirements:
\begin{enumerate}
    \item The sum of subsequent changes in \acrshort{cog} must be greater than $40$ degrees, i.e. $\sum_i |(\mathcal{X}_{t+1} - \mathcal{X}_t)| \geq 40^\circ$. This requirement ensures a straight-line trajectory.
    \item There must be sufficient data available for training in the neighborhood around the initial starting point, with similar initial heading and speed. After sanitizing the dataset and removing irrelevant trajectories, at least $3$ trajectories need to be available for training.
    \item The overall duration of the trajectories must be between $15$ and $30$ minutes.
\end{enumerate}


\section{Implementation}
The implementation details for each of the methods are described in this section.

\subsection{GP-EKF}
The GP-EKF is tested with and without the \acrshort{pdaf} update step, as well as using both finite difference and the \acrshort{cog}/\acrshort{sog} from the \acrshort{ais} dataset, resulting in $4$ different instances.

The GP-EKF required more flexibility during development. Due to the need for calculating the gradient $\frac{\delta \vec{f}}{\delta \boldsymbol{x}}$, it became impractical to use existing \acrshort{gp} implementations. Due to the simple implementation of \cref{alg:gp_prediction}, it was easier to implement it from the ground up.
The GP-EKF was therefore implemented directly in Python using only \textit{scipy}\cite{scipy} and \textit{numpy}\cite{numpy} to speed up linear algebra routines. The Cholesky decomposition in \cref{alg:gp_prediction} was computed using \texttt{scipy.linalg.cho\_factor}, which calls a highly optimized LAPACK routine. Similarily, \texttt{scipy.linalg.solve\_\-trianglular} was used to solve the lower triangular system of equations by forward substitution. The implementation of GP-EKF and the \acrshort{pdaf} update was then straightforward using \cref{alg:gp_ekf_prediction} and \cref{alg:gp_ekf_pdaf} from \cref{chap:gp_ekf}

The hyperparameters were tuned using the \acrshort{gp} implementation in the popular \textit{scikit-learn} \cite{scikit-learn} Python package. $10$ random restarts were used during optimization to reduce the risk of bad local minima. Due to the wide variety of different scenarios on the training set, the hyperparameters are optimized for each simulation iteration. This way, the robustness of the optimization is indirectly tested, as instances of bad local optima are included in the results.


The remaining parameters not found through \acrshort{ml} were tuned through trial and error on a few different trajectories.
The initial state uncertainty is set to $\boldsymbol{P}_0 = 500^2 \cdot \boldsymbol{I}$, which was found to work well during development. The \acrshort{pdaf} parameters are available in \cref{table:stats_pdaf_params}.

\begin{table}[h]
    \centering
    \begin{tabular}{lll}
        \textit{\textbf{Parameter}} &                  & \textit{\textbf{Value}}      \\ \hline
        Measurement noise           & $\boldsymbol{R}$ & $200^2 \cdot \boldsymbol{I}$ \\
        Detection Probability       & $p_D$            & $0.8$                        \\
        Clutter Rate                & $\lambda$        & $10^{-3}$                    \\
        Gate Size                   & $g$              & $2$
    \end{tabular}
    \caption{Parameters used for \acrshort{pdaf} update}
    \label{table:stats_pdaf_params}
\end{table}


\subsection{Direct GP}
The excact \acrshort{gp} formulation was implemented using the \texttt{GaussianProcessRegressor} from the popular Python library, \textit{sciki-learn} \cite{scikit-learn}. The library support all kernels introduced in \cref{chap:theory} and supports hyperparameter optimization using multiple restarts to avoid bad local optimas.


%For the \acrshort{svgp} implementation, the Python library \textit{GPFlow} \cite{GPflow2017} built on the well-known \textit{tensorflow} \cite{tensorflow2015-whitepaper} library was used. GPFlow has several implementations of approximate \acrshort{gp}s, including the \acrshort{svgp}. For optimization, the stochastic gradient descent optimizer \textit{ADAM} should be familiar for anyone working with Neural Networks. This optimizer was selected to optimize both the inducing variables as well as the hyperparameters through \textit{mini batching}. Mini Batching is an optimization technique used in combination with stochastic optimization, where a random subset of the training set is used in each iteration instead of the full dataset. This way, big datasets can be used to train the model.\todo[]{This is from memory, find a source} 

Only the exact formulation of the direct \acrshort{gp} approach is included in the following testing for two reasons:
\begin{enumerate}
    \item All methods can use the same representative trajectories for training. As a result, it becomes easier to compare the different methods as the performance is not influenced by how the training data is selected.
    \item The \acrshort{svgp} turned out to be very complicated to make work. Even after several hours of training, the model performed rather poorly, with unrealistic trajectory estimates. It was decided that it was not worth the additional effort of performing statistical testing.
\end{enumerate}

\section{Results}
The results for straight-line and curved trajectories are available in \cref{table:stats_straight_line_error} and \cref{table:stats_curved_error} respectively. \todo[]{add examples in appendix?}.

\begin{table}
    \begin{subtable}{\textwidth}
        \makebox[\textwidth][c]{
            \begin{tabular}{lllrrrrr}
                \toprule
                        &                & Time [Minutes]    & 5   & 10  & 15  & 20   & 25   \\
                Summary & Method         & Training Source   &     &     &     &      &      \\
                \midrule
                Mean    & CVM            & COG/SOG from AIS  & 235 & 473 & 756 & 1105 & 1741 \\
                        & Direct GP      & Position          & 390 & 560 & 800 & 1146 & 1693 \\
                        & GP-EKF         & COG/SOG from AIS  & 345 & 654 & 928 & 1223 & 1875 \\
                        &                & Finite Difference & 310 & 570 & 843 & 1099 & 1384 \\
                        & GP-EKF w/ PDAF & COG/SOG from AIS  & 351 & 623 & 884 & 1108 & 1507 \\
                        &                & Finite Difference & 291 & 522 & 786 & 1075 & 1159 \\
                \midrule
                Median  & CVM            & COG/SOG from AIS  & 124 & 253 & 448 & 682  & 1291 \\

                        & Direct GP      & Position          & 237 & 400 & 552 & 843  & 1274 \\
                        & GP-EKF         & COG/SOG from AIS  & 284 & 549 & 745 & 984  & 1174 \\
                        &                & Finite Difference & 256 & 490 & 677 & 867  & 834  \\
                        & GP-EKF w/ PDAF & COG/SOG from AIS  & 282 & 505 & 693 & 976  & 1144 \\
                        &                & Finite Difference & 243 & 413 & 651 & 849  & 839  \\
                \bottomrule
            \end{tabular}
        }
        \caption{Trajectory errors in meters}
        \label{table:stats_straight_traj_err}
        \vspace*{0.5cm}
    \end{subtable}
    \begin{subtable}{\textwidth}
        \makebox[\textwidth][c]{
            \begin{tabular}{lllrrrrr}
                \toprule
                        &                & Time [Minutes]    & 5   & 10  & 15  & 20  & 25   \\
                Summary & Method         & Training Source   &     &     &     &     &      \\
                \midrule
                Mean    & CVM            & COG/SOG from AIS  & 55  & 178 & 364 & 559 & 1195 \\
                        & Direct GP      & Position          & 195 & 236 & 364 & 602 & 1390 \\
                        & GP-EKF         & COG/SOG from AIS  & 133 & 240 & 343 & 394 & 636  \\
                        &                & Finite Difference & 143 & 266 & 395 & 500 & 630  \\
                        & GP-EKF w/ PDAF & COG/SOG from AIS  & 128 & 234 & 341 & 352 & 513  \\
                        &                & Finite Difference & 131 & 224 & 344 & 503 & 656  \\
                \midrule
                Median  & CVM            & COG/SOG from AIS  & 29  & 96  & 222 & 358 & 711  \\
                        & Direct GP      & Position          & 77  & 116 & 174 & 279 & 907  \\
                        & GP-EKF         & COG/SOG from AIS  & 86  & 166 & 232 & 278 & 665  \\
                        &                & Finite Difference & 90  & 171 & 258 & 375 & 507  \\
                        & GP-EKF w/ PDAF & COG/SOG from AIS  & 79  & 144 & 197 & 233 & 388  \\
                        &                & Finite Difference & 79  & 135 & 198 & 301 & 354  \\
                \bottomrule
            \end{tabular}

        }
        \caption{Path error in meters}
        \label{table:stats_straight_path_err}
    \end{subtable}
    \caption{Error summary for $350$ straight-line trajectories. Mean and median summary statistics are calculated for the trajectory and path error at fixed timestamps. Linear interpolation is used between samples. Errors for short trajectories are not extrapolated, and therefore not included in the $20$ and $25$ minute bins.}
    \label{table:stats_straight_line_error}
\end{table}

\begin{table}
    \begin{subtable}{\textwidth}
        \makebox[\textwidth][c]{
            \begin{tabular}{lllrrrrr}
                \toprule
                        &                & Time [Minutes]    & 5   & 10   & 15   & 20   & 25   \\
                Summary & Method         & Training Source   &     &      &      &      &      \\
                \midrule
                Mean    & CVM            & COG/SOG from AIS  & 599 & 1532 & 2900 & 5081 & 8349 \\
                        & Direct GP      & Position          & 202 & 311  & 421  & 523  & 500  \\
                        & GP-EKF         & COG/SOG from AIS  & 361 & 702  & 986  & 1395 & 2005 \\
                        &                & Finite Difference & 348 & 704  & 969  & 1252 & 1326 \\
                        & GP-EKF w/ PDAF & COG/SOG from AIS  & 400 & 765  & 1021 & 1065 & 1086 \\
                        &                & Finite Difference & 314 & 585  & 793  & 1050 & 1104 \\
                \midrule
                Median  & CVM            & COG/SOG from AIS  & 351 & 1101 & 2428 & 4586 & 7682 \\
                        & Direct GP      & Position          & 113 & 170  & 192  & 221  & 253  \\
                        & GP-EKF         & COG/SOG from AIS  & 281 & 529  & 821  & 1237 & 1831 \\
                        &                & Finite Difference & 214 & 448  & 756  & 1040 & 993  \\
                        & GP-EKF w/ PDAF & COG/SOG from AIS  & 283 & 471  & 619  & 785  & 704  \\
                        &                & Finite Difference & 196 & 363  & 555  & 787  & 700  \\
                \bottomrule
            \end{tabular}
        }
        \caption{Trajectory Error in meters}
        \label{table:stats_curved_traj_err}
        \vspace*{0.5cm}
    \end{subtable}
    \begin{subtable}{\textwidth}
        \makebox[\textwidth][c]{
            \begin{tabular}{lllrrrrr}
                \toprule
                        &                & Time [Minutes]    & 5   & 10  & 15   & 20   & 25   \\
                Summary & Method         & Training Source   &     &     &      &      &      \\
                \midrule
                Mean    & CVM            & COG/SOG from AIS  & 206 & 552 & 1153 & 1991 & 2854 \\
                        & Direct GP      & Position          & 124 & 184 & 242  & 310  & 304  \\
                        & GP-EKF         & COG/SOG from AIS  & 126 & 250 & 451  & 778  & 1294 \\
                        &                & Finite Difference & 169 & 274 & 360  & 420  & 417  \\
                        & GP-EKF w/ PDAF & COG/SOG from AIS  & 161 & 378 & 608  & 646  & 765  \\
                        &                & Finite Difference & 158 & 250 & 308  & 394  & 317  \\
                \midrule
                Median  & CVM            & COG/SOG from AIS  & 72  & 325 & 691  & 1708 & 2472 \\
                        & Direct GP      & Position          & 48  & 66  & 88   & 91   & 154  \\
                        & GP-EKF         & COG/SOG from AIS  & 62  & 127 & 228  & 527  & 1214 \\
                        &                & Finite Difference & 82  & 153 & 197  & 214  & 195  \\
                        & GP-EKF w/ PDAF & COG/SOG from AIS  & 71  & 121 & 197  & 304  & 337  \\
                        &                & Finite Difference & 91  & 134 & 137  & 146  & 114  \\
                \bottomrule
            \end{tabular}
        }
        \caption{Path error in meters}
        \label{table:stats_curved_path_err}
    \end{subtable}
    \caption{Error summary for $350$ curved trajectories. Mean and median summary statistics are calculated for the trajectory and path error at fixed timestamps. Linear interpolation is used between samples. Errors for short trajectories are not extrapolated, and therefore not included in the $20$ and $25$ minute bins.}
    \label{table:stats_curved_error}
\end{table}

\begin{figure}
    \centering
    \makebox[\textwidth][c]{
        \begin{subfigure}{0.65\textwidth}
            \includegraphics[width=\textwidth]{figures/straight_line_stats/gp_vs_cvm_3.pdf}
        \end{subfigure}
        \begin{subfigure}{0.65\textwidth}
            \includegraphics[width=\textwidth]{figures/straight_line_stats/gp_vs_cvm_1.pdf}
        \end{subfigure}

    }
    \caption{GP-EKF and direct GP compared to the \acrshort{cvm} method on $350$ random \textbf{straight-line} trajectories. The GP-EKF performs slightly better, though both methods perform better than the \acrshort{cvm}, especcially on long-term predictions.}
    \label{fig:stats_curved_vs_cvm}

    \vspace{0.5cm}

    \makebox[\textwidth][c]{

        \begin{subfigure}{0.65\textwidth}
            \includegraphics[width=\textwidth]{figures/curved_line_stats/gp_vs_cvm_3.pdf}
        \end{subfigure}
        \begin{subfigure}{0.65\textwidth}
            \includegraphics[width=\textwidth]{figures/curved_line_stats/gp_vs_cvm_1.pdf}
        \end{subfigure}
    }
    \caption{GP-EKF and direct \acrshort{gp} compared to the CVM method on $350$ random \textbf{curved} trajectories. Both methods perform considerly better than the \acrshort{cvm}, and the direct \acrshort{gp} performs the best with low trajectory error and low spread.}
    \label{fig:stats_straight_vs_cvm}
\end{figure}

\subsection{Straight-line trajectories}
As expected, the \acrshort{cvm} performs well on straight-line trajectories. As seen in \cref{fig:stats_straight_vs_cvm}, both the direct \acrshort{gp} approach and the GP-EKF performs slightly better than the \acrshort{cvm} on straight-line trajectories. By

\subsection{Curved Trajectories}
The GP-EKF approach performs significantly better than the \acrshort{cvm} on curved trajectories, with overall lower spread and median trajectory error as depicted in \cref{fig:stats_curved_vs_cvm}.

\begin{figure}
    \centering
    \makebox[\textwidth][c]{
        \begin{subfigure}{0.65\textwidth}
            \includegraphics[width=\textwidth]{figures/straight_line_stats/gp_vs_ekf.pdf}
            \caption{Straight-Line Trajectories}
        \end{subfigure}
        \begin{subfigure}{0.65\textwidth}
            \includegraphics[width=\textwidth]{figures/curved_line_stats/gp_vs_ekf.pdf}
            \caption{Curved Trajectories}
        \end{subfigure}
    }
    \caption{GP-EKF compared to the CVM method on $350$ random trajectories. As expected, the \acrshort{cvm} yields large trajectory errors on curved trajectories. The GP-EKF performs consistently better, with lower median error as well as lower spread. However, on straight-line trajectories, the methods behave comparably.}
    \label{fig:stats_direct_gp_vs_ekf_cvm}
\end{figure}

\begin{figure}[h]
    \centering
    \makebox[\textwidth][c]{
        \begin{subfigure}{0.65\textwidth}
            \includegraphics[width=\textwidth]{figures/straight_line_stats/gp_vs_pdaf.pdf}
            \caption{Straight-Line Trajectories}
        \end{subfigure}
        \begin{subfigure}{0.65\textwidth}
            \includegraphics[width=\textwidth]{figures/curved_line_stats/gp_vs_pdaf.pdf}
            \caption{Curved Trajectories}
        \end{subfigure}
    }
    \caption{GP-EKF with and without PDAF on curved trajectories for $350$ trajectories. While there are some slight differences, the \acrshort{pdaf} does not appear to have any considerable effect on the trajectory errors.}
    \label{fig:stats_gp_ekf_with_or_without_pdaf}

    \vspace{0.5cm}

    \makebox[\textwidth][c]{
        \begin{subfigure}{0.65\textwidth}
            \includegraphics{figures/straight_line_stats/gp_cog_vs_fd.pdf}
            \caption{Straight-Line Trajectory}
        \end{subfigure}
        \begin{subfigure}{0.65\textwidth}
            \includegraphics{figures/curved_line_stats/gp_cog_vs_fd.pdf}
            \caption{Curved Trajectory}
        \end{subfigure}
    }
    \caption{GP-EKF using finite differences and the \acrshort{cog} and \acrshort{sog} from the AIS dataset on $350$ trajectories. The finite differences approach performs consistently better, with lower median error and spread.}
    \label{fig:stats_gp_ekf_fd_vs_cog}
\end{figure}

When comparing the performance of GP-EKF with and without the update step in \cref{fig:stats_gp_ekf_with_or_without_pdaf}, there seems to be little actual gain from the added complexity of \acrshort{pdaf}.


\subsection{Finite Difference vs. COG/SOG from AIS}
\begin{figure}[h]
    \centering

\end{figure}
A key design choice for the GP-EKF is which data source to use for training. The model can either be trained using the \acrshort{cog} and \acrshort{sog} values contained in the \acrshort{ais} samples or by calculating numerical derivatives of the position through a finite-difference approach. Comparing trajectory error for both approaches on the same test set favors the finite-difference approach, as seen in \cref{fig:stats_gp_ekf_fd_vs_cog}. The finite difference approach performs consistently better, with lower median error and less spread. However, by ignoring the time component, looking at the path errors in \cref{table:stats_curved_path_err} supports the opposite, where the path errors are lower for the \acrshort{cog}/\acrshort{sog} approach. This may indicate that the issue is due to imprecise velocity estimates from the \acrshort{sog} while using the course from the \acrshort{cog} works well.


\section{Consistency}
\begin{figure}
    \centering
    \makebox[\textwidth][c]{
        \begin{subfigure}{0.65\textwidth}
            \includegraphics{figures/straight_line_stats/nees.pdf}
            \caption{Straight-Line Trajectory}
        \end{subfigure}
        \begin{subfigure}{0.65\textwidth}
            \includegraphics{figures/curved_line_stats/nees.pdf}
            \caption{Curved Trajectory}
        \end{subfigure}
    }
    \begin{subfigure}{0.65\textwidth}
        \centering
        \includegraphics{figures/curved_line_stats/direct_gp_nees.pdf}
        \caption{Direct GP NEES for curved trajectories}
        \label{fig:stats_curved_nees_direct}
    \end{subfigure}
    \caption{Boxplot of the \acrshort{nees} on straight-line and curved trajectories for each method. The dotted lines shows the theoritical quartiles ($25\%$, $50\%$ and $75\%$) for the $\mathcal{X}^2$ distribution as a comparison.}.
    \label{fig:stats_curved_nees}
\end{figure}
The estimated quartiles \acrshort{nees} for straight-line and curved trajectories is displayed for each method in \cref{fig:stats_curved_nees}.

On straight-line trajectories, the $25\%$ and $50\%$ quartiles correspond well with the theoretical values for the $\mathcal{X}^2$ distribution for all methods. The \acrshort{pdaf} update step does, however, reduce the predicted uncertainty, leading to increased overconfidence. In addition, the estimated \acrshort{nees} distribution has a much fatter right-tail than the theoretical $\mathcal{X}^2$ distribution, leading to an estimated $75\%$ quartile far outside the theoretical values.

On curved trajectories, the GP-EKF with and without PDAF is consistently overconfident. The theoretical quartiles are not plotted due to the large y-axis. The direct \acrshort{gp} approach performs significantly better, which is why a separate plot in \cref{fig:stats_curved_nees_direct} is added to compare it to theoretical quartiles.




