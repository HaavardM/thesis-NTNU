\chapter{Neccessary Theoretical Background}

\section{Useful Results From Probability Theory}

The reader is assumed to have basic understanding of probability theory. Some of the most relevant results are summarized here.
 
 \subsection{Notation}
 
 The notation $p(X)$ is used to denote the probability distribution of the random variable $X$.
 
 For \textbf{discrete} random variables the notation is straight forward. 
 \begin{align*}
 p(X) &= p(X=x)\\  &= p_X(X=x)\\ &= \Pr\{X = x\}
 \end{align*}
 
 For \textbf{continuous} random variables the probability of a single outcome is always zero, i.e. $p(X = x) \equiv 0$, as there are infinitely many possible outcomes. The notation $p(X)$ then denotes the probability density function (PDF) of $X$. 
 
 
 
\subsection{Joint Probabilities}
The \textit{joint probability} $p(X, Y)$ is the probability that both $X$ and $Y$ occurs \cite[p.~29]{murphy}.

\begin{equation}
    p(X, Y) = p(X \cap Y) = p(X | Y)p(Y)
\end{equation}

\subsection{Conditional Probabilities}
The \textit{conditional probability} $p(X | Y)$ is the probability of $X$ occurring, given the known occurrence of another event $Y$. This can be interpreted as knowing the value of $Y$ includes some information about $X$. Mathematically it can be expressed as \cite[p.~29]{murphy}
\begin{equation}\label{eq:conditional_probability}
    p(X | Y) = \frac{p(X, Y)}{p(Y)}
\end{equation}

\subsection{Bayes Rule}

A useful extension to equation \eqref{eq:conditional_probability} is to recognize that the joint distribution $p(X, Y)$ can be rewritten as a product of a conditional probability $p(Y | X)$ and $p(X)$. Inserting into equation \eqref{eq:conditional_probability} yields \textit{Bayes Rule}
\begin{equation}\label{eq:bayes_law}
    p(X | Y) = \frac{p(X, Y)}{p(Y)} = \frac{p(Y | X)p(X)}{p(Y)}.
\end{equation}

As $Y$ is known, the denominator $p(Y)$ is simply a normalizing constant. It is sometimes useful to rewrite equation \eqref{eq:bayes_law} as
\begin{equation}\label{eq:bayes_law_proportional}
    p(X | Y) = \frac{p(Y | X) p(X)}{p(Y)} \propto p(Y | X)p(X)
\end{equation} 
if $p(Y)$ is hard to calculate and the normalized value of $p(X | Y)$ is not needed.

\subsection{Marginal Probability \& The Law of Total Probability}\label{sec:marginal_prob}
The \textit{marginal probability} of an event $X$ is the probability of $X$ occurring irrespective of any other variables.
For notational simplicity the integral operator is used for marginalization of both continuous and discrete random variables, even though the integral is replaced by a sum for discrete random variables. For an event $X$ and any other variables $\bf Y$, the marginal probability of $X$ can be written as
\begin{equation}
    p(X) = \int_{\boldsymbol{Y}} p(X, \boldsymbol{Y}) d\boldsymbol{Y} = \int_{\boldsymbol{Y}} p(X | \boldsymbol{Y}) p(\boldsymbol{Y}) d\boldsymbol{Y}
\end{equation}
The last equality is by the \textit{Law of total probability}, which relates marginal probabilities to conditional probabilities.

\subsection{Independence \& Conditional Independence}
If the joint probability of two variables $X$ and $Y$ can be expressed as a product of two marginals, then they are \textit{marginally independent}.
\begin{equation}
    X \perp Y \iff p(X, Y) = p(X | Y)p(Y) = p(Y | X)p(X) = p(X)p(Y)
\end{equation}

Marginal independence is rare, as most variables usually influence each other in some way. However, the variables often affect one another indirectly through other variables. The variables $X$ and $Y$ are said to be \textit{conditionally independent} given $Z$, if the conditional joint can be written as a product of conditional marginals \cite[p.~31]{murphy}:
\begin{equation}\label{eq:conditional_independence}
    X \perp Y | Z \iff p(X, Y | Z) = p(X | Z)p(Y | Z)
\end{equation}

\subsection{Interpretations of Probability}
The results mentioned so far stem from abstract mathematical axioms, and do not tell how to interpret the resulting probabilities. Different interpretations are commonly accepted. The perhaps two biggest interpretations are the Frequentist and Bayesian interpretations. 

\begin{description}
    \item[The Frequentist Interpretation:] The Frequentists define an event's probability as the limit of its relative frequency over many trials. In other words, the probabilities are assigned a physical interpretation and remains rather objective. There do however arise issues and paradoxes when assigning probabilities to events which are not recurrent, i.e. they only happen a few times. The Frequentist interpretation assumes that the collected data is random and that the model (and its corresponding parameters) are fixed. The main goal of the Frequentists are therefore to create consistent methods for dealing with uncertain data.
    \item[The Bayesian Interpretation:] The Bayesians interpret probability as a state of knowledge \cite{Jaynes86bayesianmethods:}. In Bayesian analysis the data is fixed, whereas the model is unknown. Data is used to update prior knowledge about the model, and the probabilities are used to quantify how strongly one believe in each outcome. This interpretation is highly philosophical, but beautifully captures humans' intuitive reasoning. The Bayesian interpretation does however involve a level of subjectivity when choosing priors, making it difficult to form objective opinions from data. For those interested, see \Cite{Jaynes86bayesianmethods:} for a fascinating read on the history of Bayesian probability.
\end{description}

While the differences between the Frequentist and Bayesian interpretations are mostly philosophical, there are a few practical differences. For a Frequentist it does not make sense to talk about any probabilities before an experiment has been performed. The prior $p(X)$ and posterior $p(X | Y)$ is therefore nonsensical and cannot be computed using a Frequentist interpretation.






\section{Bayesian Statistics}

Using \cref{eq:bayes_law} one can write 
\begin{equation}\label{eq:bayes_learning}
    p(\boldsymbol{\theta}| \mathcal{D}, \boldsymbol{\eta}) = \frac{p(\mathcal{D} | \boldsymbol{\theta}) p(\boldsymbol{\theta} | \boldsymbol{\eta})}{p(\mathcal{D})} \propto p(\mathcal{D} | \boldsymbol{\theta})p(\boldsymbol{\theta} | \boldsymbol{\eta})
\end{equation}

If $\boldsymbol{\theta}$ is the unknown parameters of a process, $\mathcal{D}$ is collected data or observations, and $\boldsymbol{\eta}$ is all prior knowledge about $\boldsymbol{\theta}$, then equation \eqref{eq:bayes_learning} is a mathematical representation of the process of learning from data \cite{Jaynes86bayesianmethods:}.

\cref{eq:bayes_learning} can be interpreted as
\begin{description}
    \item[The Prior $p(\boldsymbol{\theta} | \boldsymbol{\eta})$:] The prior incorporates knowledge about $\boldsymbol{\theta}$ before observing any data. This can be domain-specific knowledge, results from prior experiments or intuitive reasoning about possible values of $\boldsymbol{\theta}$. 
    \item[The Likelihood $p(\mathcal{D} | \boldsymbol{\theta})$]: The likelihood of the observations $\mathcal{D}$ is how well the observations fit with prior beliefs $\boldsymbol{\theta} | \boldsymbol{\eta}$. In other words, how likely it is to observe $\mathcal{D}$ if the current belief $\boldsymbol{\theta}$ were to be true.
    \item[The Posterior $p(\boldsymbol{\theta} | \mathcal{D}, \boldsymbol{\eta})$]: The posterior distribution is the updated belief about $\boldsymbol{\theta}$. This is knowledge about $\boldsymbol{\theta}$ after observing the data. 
\end{description}

The conditional variable $\boldsymbol{\eta}$ is usually omitted for simplified notation, but is implicitly defined through the choice of prior distribution, i.e. $p(\boldsymbol{\theta}) = p(\boldsymbol{\theta} | \boldsymbol{\eta})$


\subsection{Choice of prior}

\begin{description}
\item[The Bayesian Approach:]As the prior $\boldsymbol{\eta}$ can be hard to determine, the Bayesian approach is to define priors on priors. This is called a hierarchical Bayesian model and allows for complex models with multiple dependent variables affecting each other through priors \cite{murphy}. The relation between the variables can be represented as a graphical model:

\begin{figure*}[h!]
\centering    
\begin{tikzpicture}
    \node[latent] (e) {$\boldsymbol{\eta}$};
    \node[latent, right=of e] (t) {$\boldsymbol{\theta}$};
    \node[obs, right=of t] (d) {$\mathcal{D}$};
    \edge {e} {t}
    \edge{t} {d}
\end{tikzpicture}
\end{figure*}

\item[Uninformative Priors:] An uninformative prior is a distribution which does not favor any outcome, and thereby does not incorporate any prior knowledge. It is like saying one simply does not know what to believe.
\item[Empirical Bayes:] The priors can be estimated from the data, resulting in the so-called Empirical Bayes method. The parameters of the prior can be found by maximizing the conditional likelihood. %TODO: cite
\end{description}


\section{Stochastic Modelling}

\subsection{Markov Chains}
A Markov Chain is a chain of events, where the outcome of the next event only depends on the current state. All information needed to predict the future is contained in the current state. This property is called the \textit{Markov Property} and is expressed in \cref{eq:theory_markov_property}.

\begin{equation}\label{eq:theory_markov_property}
p(\mathbf{X}_{t+1} | \mathbf{X}_t,  \mathbf{X}_{0:t-1}) = p(\mathbf{X}_{t+1} | \mathbf{X}_t)  \quad \forall t \in [1, \infty)
\end{equation}

\subsubsection{Stationary Distribution}
As time moves on, some states will be visited more frequently than others. This long-running distribution of states is called the \textit{stationary distribution} of the Markov Chain. If $\mathbf{P}$ is the transition probability matrix for a discrete Markov chain, then $\boldsymbol{\pi}$ is the stationary distribution if 
\begin{equation}
    \boldsymbol{\pi} = \boldsymbol{\pi} \mathbf{P}
\end{equation}

The stationary distribution may not be unique, and whether a unique stationary distribution exists, depends on how the Markov Chain behaves. Further details on stationary distributions are outside the scope of this thesis. 



\section{Probabilistic Graphical Models}
\textit{\acrfull{pgm}}, often called Bayesian Networks, allows for efficient factorization of the joint distribution by assuming conditional independence, as defined in \cref{eq:conditional_independence}, between variables. \acrshort{pgm}'s are graphs, where nodes represent random variables and edges represents statistical dependence between the variables. By utilizing the independence assumptions in the graphs, the computational complexity can be drastically reduced \cite{murphy}.

\subsection{\acrfull{dgm}}
\acrshort{dgm}'s are perhaps the simplest type of \acrshort{pgm} and is based on \textbf{Directed Acyclic Graphs} (DAG). It is a graph structure which do not allow cycles and all edges are directed. The flow of information is explicitly modeled in the direction on the edges. \acrshort{dgm}'s are well-suited for modelling causal relationships and when the flow of information is clearly directed. An example can be the relationship between the state of a physical system and a sensor. The system affects the sensor, but the sensor does not affect the system. For the \acrshort{dgm} in \cref{fig:dgm}, the variables $B$ and $C$ are conditionally independent given $A$, i.e. $B \perp C | \; A$. However, knowing $D$ restricts $C$ and $B$, i.e. $B \not\perp C | \; D$. This all comes directly from inspecting the graph. 

\acrshort{dgm} allows for straight-forward factorization of the joint-distribution 
\begin{equation}\label{eq:dgm_factorization}
    p(\mathbf{x}) = \prod_{v \in \mathcal{V}}p(v | \mathbf{pa}(v))
\end{equation}
where $\mathcal{V}$ is all the nodes in the graph and $\mathbf{pa}(v)$ is the parent's for node $v$. 

\subsection{\acrfull{mrf}}

\acrshort{mrf}'s are undirected graphical models, and offers a more general specification of independence assumptions. They are in some domains, such as relational or spatial data , more natural than \acrshort{dgm}'s as they are symmetric, i.e. the information flows both ways \cite{murphy}. The \acrshort{mrf} in \cref{fig:mrf} shows that $B$ and $C$ is only independent if both $A$ and $D$ is observed, i.e. $B \perp C | \; A, D$. The important thing to notice is that the \acrshort{mrf} makes vastly different assumptions than the \acrshort{dgm}, even though the structure is similar.

By the \textbf{Hammersley-Clifford Theorem} \cite[p.~ 668]{murphy}, the joint distribution of \acrshort{mrf} models can be factorized on the form
\begin{equation}
    p(\mathbf{x}) =\frac{1}{Z} \prod_{c \in \mathcal{C}} \psi_c(\mathbf{x}_c)
\end{equation}
where is $\mathcal{C}$ is the set of all \textit{maximal cliques}, i.e. largest, fully connected sub graphs. All variables in a maximal clique is therefore dependent on each other. $\psi_c(\cdot)$ is the \textit{(factor) potential function} for the nodes in the clique $c$, and determines the strength of the interaction between the variables $\bf{x}_c$. The \textit{(factor) partition function}
\begin{equation}
    Z = \int_\mathbf{x} (\prod_{c \in \mathcal{C}} \psi_c(\mathbf{x}_c)) d \mathbf{x}
\end{equation}
is a normalization constant so that $p(\mathbf{x}) \in [0, 1]$. The potential functions $\psi_c(\cdot)$Â can therefore be arbitrary non-negative functions. 

\acrshort{dgm}'s can be converted into \acrshort{mrf}'s through \textit{\gls{moralization}}. \Gls{moralization} is performed by making all edges undirected and add new edges between variables sharing a child. Some independence information is lost in the process, but it represent the same factorization of the joint distribution. The computational complexity of inference on \acrshort{dgm}s and \acrshort{mrf}s are, generally speaking, the same  \cite{murphy}. Inference methods for \acrshort{mrf}'s can therefore also be used on \acrshort{dgm}'s by first moralizing.


\subsection{Factor Graphs}
Factor Graphs unifies the concept of directed (\acrshort{dgm}) and undirected graphical models (\acrshort{mrf}). A factor graph is an undirected bipartite graph. Bipartite graphs are graphs with two different types of nodes, factors and variables. All variables are represented by round nodes and all factors are square nodes. Each factor is connected to all variables it references through undirected edges. Both \acrshort{mrf}'s and \acrshort{dgm}'s can be converted to factor graphs. Factor graphs is not really intended for modelling independence, but rather visualize how the joint distribution can be factorized to allow for efficient computations. \cref{fig:factor} shows a factor graph for the \acrshort{mrf} in \cref{fig:mrf}.

For generative models, an extension to factor graph notation is proposed in \cite{dietz} to allow for a more intuitive reasoning about the model. The directed edges in \cref{fig:factor_directed} contains additional information about how realizations of the random variables are generated, similar to the \acrshort{dgm} in \cref{fig:dgm}. The factors can the be viewed intuitively as generative processes, with input and output variables. It otherwise contain the exact same functional representation as the normal factor graph in \cref{fig:factor}. 

\begin{figure}[h]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\begin{tikzpicture}
    \node[latent] (A) {$A$};
    \node[latent, right=of A] (B) {$B$};
    \node[latent, below=of A] (C) {$C$};
    \node[latent, below=of B] (D) {$D$};
    \edge[-] {A} {B};
    \edge[-] {A} {C};
    \edge[-] {C} {D}
    \edge[-] {B} {D}
\end{tikzpicture}
\caption{\acrfull{mrf}}
\label{fig:mrf}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\begin{tikzpicture}
    \node[latent] (A) {$A$};
    \node[latent, right=of A] (B) {$B$};
    \node[latent, below=of A] (C) {$C$};
    \node[latent, below=of B] (D) {$D$};
    \edge {A} {B};
    \edge {A} {C};
    \edge {C} {D}
    \edge {B} {D}
\end{tikzpicture}
\caption{\acrfull{dgm}}
\label{fig:dgm}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\begin{tikzpicture}
    \node[latent] (A) {$A$};
    \node[latent, right=of A] (B) {$B$};
    \node[latent, right=of B] (C) {$C$};
    \node[latent, right=of C] (D) {$D$};
    \factor[below=of A] {ba-factor} {} {} {};
    \factor[below=of B] {ca-factor} {} {} {};
    \factor[below=of C] {dbc-factor} {} {} {};
    \factoredge{A, B}{ba-factor}{}
    \factoredge{A, C}{ca-factor}{}
    \factoredge{B, C, D}{dbc-factor}{}
\end{tikzpicture}
\caption{Factor Graph}
\label{fig:factor}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\begin{tikzpicture}
    \node[latent] (A) {$A$};
    \node[latent, right=of A] (B) {$B$};
    \node[latent, right=of B] (C) {$C$};
    \node[latent, right=of C] (D) {$D$};
    \factor[below=of A] {ba-factor} {} {} {};
    \factor[below=of B] {ca-factor} {} {} {};
    \factor[below=of C] {dbc-factor} {} {} {};
    \factoredge{A}{ba-factor}{B}
    \factoredge{A}{ca-factor}{C}
    \factoredge{B}{dbc-factor}{D}
    \factoredge{C}{dbc-factor}{D}
\end{tikzpicture}
\caption{Factor Graph for generative model}
\label{fig:factor_directed}
\end{subfigure}

\caption{\acrshort{pgm} representations of a probabilistic models with $4$ variables.}
\end{figure}

\section{Probability Distributions}
\subsection{Conjugate Priors}\label{sec:theory_conjugate_priors}
If the posterior distribution is in the same family as the prior, then the posterior is called the \textit{conjugate distribution} and the prior is called the \textit{conjugate prior}. Using conjugate priors in Bayesian Statistics, allows for analytically tractable inference as the posterior becomes a well known probability distribution. \cref{table:conjugate_priors} summarizes the conjugate priors for a few common distribution.


\begin{table}[h]
\begin{tabular}{lllll}
\hline
\multicolumn{1}{|l|}{\textbf{Distribution}} & \multicolumn{1}{l|}{\textbf{1st Parent}}        & \multicolumn{1}{l|}{\textbf{Conjugate}} & \multicolumn{1}{l|}{\textbf{2nd Parent}} & \multicolumn{1}{l|}{\textbf{Conjugate}} \\ \hline
\multicolumn{1}{|l|}{Gaussian}              & \multicolumn{1}{l|}{mean $\mu$}                 & \multicolumn{1}{l|}{Gaussian}           & \multicolumn{1}{l|}{precision $\gamma$}  & \multicolumn{1}{l|}{Gamma}              \\ \hline
\multicolumn{1}{|l|}{Gamma}                 & \multicolumn{1}{l|}{shape $\alpha$}             & \multicolumn{1}{l|}{none}               & \multicolumn{1}{l|}{scale $b$}           & \multicolumn{1}{l|}{Gamma}              \\ \hline
\multicolumn{1}{|l|}{Discrete}              & \multicolumn{1}{l|}{probabilities $\mathbf{p}$} & \multicolumn{1}{l|}{Dirichlet}          & \multicolumn{1}{l|}{parents $\{x_i\}$}   & \multicolumn{1}{l|}{Discrete}           \\ \hline
\multicolumn{1}{|l|}{Dirichlet}             & \multicolumn{1}{l|}{pseudo-counts $\bf a$}      & \multicolumn{1}{l|}{none}               & \multicolumn{1}{l|}{}                    & \multicolumn{1}{l|}{}                   \\ \hline
\multicolumn{1}{|l|}{Exponential}           & \multicolumn{1}{l|}{scale $\alpha$}             & \multicolumn{1}{l|}{Gamma}              & \multicolumn{1}{l|}{}                    & \multicolumn{1}{l|}{}                   \\ \hline
\multicolumn{1}{|l|}{Poisson}               & \multicolumn{1}{l|}{mean $\lambda$}             & \multicolumn{1}{l|}{Gamma}              & \multicolumn{1}{l|}{}                    & \multicolumn{1}{l|}{}                   \\ \hline
\end{tabular}
\caption{Table of conjugate priors for common, standard distributions. For cells with \textit{none}, there exist no standard distribution as a conjugate prior. \cite[p.~676]{winnbishop}}
\label{table:conjugate_priors}
\end{table}


\subsection{Beta \& Dirichlet Distributions}
\subsection{Binomial \& Multinomial Distributions}
\subsection{Dirichlet-Multinomial Distribution}
\subsection{Bernouilli \& Categorial Distributions}
\subsection{Normal Distribution}
\subsection{The Exponential Family}
The Exponential Family is a family of distribution for which the \acrshort{pdf} or \acrshort{pmf} can be expressed on the from
\begin{equation}
    p(\mathbf{x} | \boldsymbol{\theta}) = h(\mathbf{x}) \exp[\eta(\boldsymbol{\theta}) T(\mathbf{x}) - A(\boldsymbol{\theta})]
\end{equation}
All the distributions mentioned so far are part of the Exponential Family. Distributions in the exponential family always have conjugate priors, however they may not be standard, well-known distributions. %TODO CITE
