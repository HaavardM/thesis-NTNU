
\chapter{Monte Carlo Sampling}
\section{Importance Sampling}\label{sec:theory_importance_sampling}

\section{Markov-Chain-Monte-Carlo}
The main idea behind \acrfull{mcmc} algorithms is to construct Markov Chains whose stationary distribution is the desired target density. After drawing many correlated samples from such a chain, the fraction of time spent in each state will be proportional to the target density $p^*(\mathbf{x})$ \cite{murphy}. 

\subsection{Metropolis Hastings Algorithm}
The Metropolis Hastings Algorithm (MH) is described in \cite[p.~850]{murphy}.
For each current state $\mathbf{x}$, a new state $\mathbf{x'}$ is proposed according to a proposal density $q(\mathbf{x'} | \mathbf{x})$. The proposal density can be chosen freely as long as it gives a non-zero probability of moving to states with non-zero probability in the target density. In other words, $q(\cdot)$ must cover all possible realizations of $p^*(\cdot)$.Â 

After proposing a move to $\mathbf{x'}$, the proposal is either \textbf{rejected} or \textbf{accepted} according to a formula which ensures that the stationary distribution of the Markov chain equals the target distribution. 

For a \textbf{symmetric} proposal distribution, i.e. $q(\mathbf{x'} | \mathbf{x}) = q(\mathbf{x}| \mathbf{x'})$, the acceptable probability is given by 
\begin{equation}
    r = \min(1, \frac{p^*(\mathbf{x'})}{p^*(\mathbf{x})})
\end{equation}

For an \textbf{asymmetric} proposal, i.e. $q(\mathbf{x'} | \mathbf{x}) \neq q(\mathbf{x} | \mathbf{x'})$, the \textit{Hastings correction} is needed to compensate for any bias introduced by $q(\cdot)$
\begin{subequations}.
\begin{align}
    r &= \min(1, \alpha)\\
    \alpha &= \frac{p^*(\mathbf{x'}) q(\mathbf{x'} | \mathbf{x})}{p^*(\mathbf{x}) q(\mathbf{x}| \mathbf{x'})}\label{eq:mcmc_mh_acceptance}
\end{align}
\end{subequations}

The normalization constant in the target density $p^*(\cdot)$ will cancel in equation \eqref{eq:mcmc_mh_acceptance}. The MH algorithm can therefore be used with unnormalized distributions, making it a very useful algorithm for sampling from complex distributions. 

The MH algorithm is summarized in \cref{alg:metropolis_hastings}.
\begin{algorithm}
\SetAlgoLined
\For(){t = 0, 1, 2}{
    Sample $x' \sim q(x' | x_t)$ \;
    Compute acceptance probability \\
    $\alpha = \frac{p^*(\mathbf{x'}) q(\mathbf{x'} | \mathbf{x}_t)}{p^*(\mathbf{x}_t) q(\mathbf{x}_t | \mathbf{x'})}$\;
    Compute $r = \min(1, \alpha)$\;
    Sample $u \sim \mathcal{U}(0, 1)$\;
    Set $x_{t+1} = \begin{cases}x' & \text{if } u \leq r\\x_t & \text{otherwise}\end{cases}$;
}
\caption{Metropolis Hastings}
\label{alg:metropolis_hastings}
\end{algorithm}

\subsection{Random Walk Metropolis}\label{sec:random_walk_metropolis}
A common proposal density is a symmetric Gaussian distribution centered at the current state, i.e. $$q(\mathbf{x'} | \mathbf{x}) = \mathcal{N}(\mathbf{x'} | \mathbf{x}, \boldsymbol{\Sigma})$$ resulting in the \textit{Random Walk Metropolis Algorithm}.

\subsection{Independence Sampling}
If the proposal density $q(\cdot)$ is independent of the current state, i.e. $$q(\mathbf{x'} | \mathbf{x}) = q(\mathbf{x'})$$ then the MH algorithm boils down to the \textit{independence sampler}, which is similar to importance sampling as described in \cref{sec:theory_importance_sampling} \cite{murphy}.

\subsection{Hamiltonian MCMC}

Sampling in a continous state-space using \cref{alg:metropolis_hastings} with a simple proposal distribution can be slow. If the gradients of the (unnormalized) target distribution is known, other methods such as Hamiltonian MCMC can be used to drastically speed up the sampling process by proposing new states with increased likelihood of acceptance. In Hamiltonian MCMC (HMC) the parameters of the model are viewed as the position $\bf q$ of a physical particle. Auxiliary variables are added to describe the momentum $\bf p$. The Hamiltonian Dynamics in \cref{eq:hamiltonian_dynamics} are then simulated and used to propose new states with high acceptance probability \cite{neal2012mcmc,murphy,hoffman2011nouturn}. HMC requires a few parameters to be specified. The number of \textbf{leapfrog step} describes how many steps should be simulated for each proposal, while the \textbf{step size} is the size of the discretization used when simulating the Hamiltonian Dynamics. Especcially the number of leapfrog step can be difficult to tune, as too many leads to unceccessary computations while too few leads to random walk behaviour \cite{hoffman2011nouturn}. 

\begin{align}\label{eq:hamiltonian_dynamics}
    \frac{d q_i}{dt} &= \frac{\partial H}{\partial p_i} & \frac{d p_i}{dt} &= -\frac{\partial H}{\partial q_i}
\end{align}

An extension to HMC, the \textbf{No-U-Turn sampler} (NUTS), is proposed in \cite{hoffman2011nouturn}. It eliminates the need for manual tuning of leapfrog steps and it is shown empirically to perform comparably to well tuned HMC method \cite{hoffman2011nouturn} without any manual intervention. Further details of the HMC and NUTS sampler are outside the scope of this thesis.   



\subsection{Burn-in}
The samples from the Markov Chain are not from the target distribution until the chain reaches its stationary distribution. A (potentially large) number of samples on the beginning of the chain are therefore invalid and are usually discarded. This is called the \textbf{burn-in phase} and is one of the major weaknesses of \acrshort{mcmc} \cite{murphy}. Convergence of a Markov Chain is difficult to detect and in practice a fixed, large number of samples are discarded.   

It can be shown that if the stationary distribution of a Markov Chain exists, it will be independent of starting state \cite{murphy}. Initializing the chain at different points will not affect the stationary distribution and convergence can be verfied by comparing independently sampled chains, initialized with different values. If all chains have converged toward the same value, the burn-in phase is complete.    
%TODO Add trace plot

\subsection{MCMC in practice}

\subsection{Convergence Guarantees}

\subsection{Bijectors}
Sampling from a constrained distribution (i.e. a Beta distribution which only offer support on $x \in (0, 1)$) using an unconstrained proposal distribution can quickly take a long time as many of the proposal will be rejected. A solution to this problem is to transform the proposal distribution such that the MCMC algorithm can sample in an unconstrained space \cite{Parno_2018, tensorflow2015-whitepaper}. 

The framework \textit{Tensorflow Probability} introduce the concept of \textit{Bijectors} which are predefined (potentially non-linear) transformations of distributions. Softmax, Softplus, Sigmoid, Affine and Exponential are among many supported transformations. Using Bijectors, the proposal distributions in Tensorflow Probability can easily be transformed into a unconstrained space without dealing with any error-prone calculations \cite{tensorflow2015-whitepaper}. 


