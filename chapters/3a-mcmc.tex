
\chapter{Monte Carlo Sampling}
\section{Importance Sampling}\label{sec:theory_importance_sampling}

\section{Markov-Chain-Monte-Carlo}
The main idea behind \acrfull{mcmc} algorithms is to construct Markov Chains whose stationary distribution is the desired target density. After drawing many correlated samples from such a chain, the fraction of time spent in each state will be proportional to the target density $p^*(\mathbf{x})$ \cite{murphy}. 

\subsection{Metropolis Hastings Algorithm}
The Metropolis Hastings Algorithm (MH) is described in \cite[p.~850]{murphy}.
For each current state $\mathbf{x}$, a new state $\mathbf{x'}$ is proposed according to a proposal density $q(\mathbf{x'} | \mathbf{x})$. The proposal density can be chosen freely as long as it gives a non-zero probability of moving to states with non-zero probability in the target density. In other words, $q(\cdot)$ must cover all possible realizations of $p^*(\cdot)$.Â 

After proposing a move to $\mathbf{x'}$, the proposal is either \textbf{rejected} or \textbf{accepted} according to a formula which ensures that the stationary distribution of the Markov chain equals the target distribution. 

For a \textbf{symmetric} proposal distribution, i.e. $q(\mathbf{x'} | \mathbf{x}) = q(\mathbf{x}| \mathbf{x'})$, the acceptable probability is given by 
\begin{equation}
    r = \min(1, \frac{p^*(\mathbf{x'})}{p^*(\mathbf{x})})
\end{equation}

For an \textbf{asymmetric} proposal, i.e. $q(\mathbf{x'} | \mathbf{x}) \neq q(\mathbf{x} | \mathbf{x'})$, the \textit{Hastings correction} is needed to compensate for any bias introduced by $q(\cdot)$
\begin{subequations}.
\begin{align}
    r &= \min(1, \alpha)\\
    \alpha &= \frac{p^*(\mathbf{x'}) q(\mathbf{x'} | \mathbf{x})}{p^*(\mathbf{x}) q(\mathbf{x}| \mathbf{x'})}\label{eq:mcmc_mh_acceptance}
\end{align}
\end{subequations}

The normalization constant in the target density $p^*(\cdot)$ will cancel in equation \eqref{eq:mcmc_mh_acceptance}. The MH algorithm can therefore be used with unnormalized distributions, making it a very useful algorithm for sampling from complex distributions. 

The MH algorithm is summarized in \cref{alg:metropolis_hastings}.
\begin{algorithm}
\SetAlgoLined
\For(){t = 0, 1, 2}{
    Sample $x' \sim q(x' | x_t)$ \;
    Compute acceptance probability \\
    $\alpha = \frac{p^*(\mathbf{x'}) q(\mathbf{x'} | \mathbf{x}_t)}{p^*(\mathbf{x}_t) q(\mathbf{x}_t | \mathbf{x'})}$\;
    Compute $r = \min(1, \alpha)$\;
    Sample $u \sim \mathcal{U}(0, 1)$\;
    Set $x_{t+1} = \begin{cases}x' & \text{if } u \leq r\\x_t & \text{otherwise}\end{cases}$;
}
\caption{Metropolis Hastings}
\label{alg:metropolis_hastings}
\end{algorithm}

\subsection{Random Walk Metropolis}\label{sec:random_walk_metropolis}
A common proposal density is a symmetric Gaussian distribution centered at the current state, i.e. $$q(\mathbf{x'} | \mathbf{x}) = \mathcal{N}(\mathbf{x'} | \mathbf{x}, \boldsymbol{\Sigma})$$ resulting in the \textit{Random Walk Metropolis Algorithm}.

\subsection{Independence Sampling}
If the proposal density $q(\cdot)$ is independent of the current state, i.e. $$q(\mathbf{x'} | \mathbf{x}) = q(\mathbf{x'})$$ then the MH algorithm boils down to the \textit{independence sampler}, which is similar to importance sampling as described in \cref{sec:theory_importance_sampling} \cite{murphy}.

\subsection{Hamiltonian MCMC}

Sampling in a continuous state-space using \cref{alg:metropolis_hastings} with a simple proposal distribution can be slow. If the gradients of the (unnormalized) target distribution is known, other methods such as Hamiltonian MCMC can be used to drastically speed up the sampling process by proposing new states with increased likelihood of acceptance. In addition to the model parameters $\mathbf{q}$, auxiliary momentum variables $\bf p$ are sampled and used to simulate the state as a particle moving around in space. The Hamiltonian Dynamics in \cref{eq:hamiltonian_dynamics} are simulated and used to propose new states with high acceptance probability \cite{neal2012mcmc,murphy,hoffman2011nouturn,robert2018accelerating}. HMC requires a few parameters to be specified. The number of \textbf{leapfrog step} describes how many steps should be simulated for each proposal, while the \textbf{step size} is the size of the discretization used when simulating the Hamiltonian Dynamics. Especially the number of leapfrog step can be difficult to tune, as too many leads to unnecessary computations while too few leads to random walk behaviour \cite{hoffman2011nouturn}. 

Hamilton's equations is given by \cref{eq:hamiltonian_dynamics}. $H$ is the Hamiltonian representing the total energy of the system as a function of position and momentum, usually in the form of the potential energy $U(\mathbf{q})$ and the kinetic energy as expressed in \cref{eq:hamiltonian}. In HMC the potential energy $U(\mathbf{q})$ is selected to be the negative log likelihood of the (unnormalized) target distribution and $M$ is a positive definite mass matrix, typically diagonal or a scalar multiple of the identity matrix \cite{neal2012mcmc}. 
\begin{align}\label{eq:hamiltonian_dynamics}
    \frac{d q_i}{d\tau} &= \frac{\partial H(\mathbf{q}, \mathbf{p})}{\partial p_i} & \frac{d p_i}{d\tau} &= -\frac{\partial H(\mathbf{q}, \mathbf{p})}{\partial q_i}
\end{align}

\begin{align}\label{eq:hamiltonian}
    H(\mathbf{q}, \mathbf{p}) &= U(\mathbf{q}) + \frac{1}{2} \mathbf{p}^\intercal M^{-1} \mathbf{p} & U(\mathbf{q}) = - \ln p^*(\mathbf{q})
\end{align}

An extension to HMC, the \textbf{No-U-Turn sampler} (NUTS), is proposed in \cite{hoffman2011nouturn}. It eliminates the need for manual tuning of leapfrog steps and it is shown empirically to perform comparably to well tuned HMC method without any manual intervention \cite{hoffman2011nouturn}.



\subsection{Burn-in}
The samples from the Markov Chain are not from the target distribution until the chain reaches its stationary distribution. A (potentially large) number of samples on the beginning of the chain are therefore invalid and are usually discarded. This is called the \textbf{burn-in phase} and is one of the major weaknesses of \acrshort{mcmc} \cite{murphy}. Convergence of a Markov Chain is difficult to detect and in practice a fixed, large number of samples are discarded.   

It can be shown that if the stationary distribution of a Markov Chain exists, it will be independent of starting state \cite{murphy}. Initializing the chain at different points will not affect the stationary distribution and convergence can be verified by comparing independently sampled chains, initialized with different values. If all chains have converged toward the same value, the burn-in phase is complete.    
%TODO Add trace plot

\subsection{MCMC in practice}
Many software packages are available for \acrshort{mcmc}, such as: 

\begin{enumerate}
    \item Tensorflow Probability \cite{tensorflow2015-whitepaper}
    \item STAN \cite{stan}
    \item PyMC3 \cite{pymc3}
\end{enumerate}

In practice, these packages can handle much of the complexity performing MCMC based inference, leaving only the model specification and a few parameters to the user. These packages also handles implementation details such as efficient utilization of available hardware (GPU and CPU) and automatic differentiation. Accelerated sampling methods such as HMC and NUTS are also easy to use in these software packages.   

\subsection{Thinning}
As new states are proposed from previous states, the samples are naturally highly autocorrelated. This results in a lot of samples which provide little additional information about the true distribution. A simple method to reduce the autocorrelation and thereby improve the information the samples is to use \textbf{thinning}, were only the n'th samples are kept. This can save a lot fo storage \cite{murphy}.

\subsection{Convergence Guarantees}

\subsection{Bijectors}
Sampling from a constrained distribution (i.e. a Beta distribution which only offer support on $x \in (0, 1)$) using an unconstrained proposal distribution can quickly take a long time as many of the proposal will be rejected. A solution to this problem is to transform the proposal distribution such that the MCMC algorithm can sample in an unconstrained space \cite{Parno_2018, tensorflow2015-whitepaper}. 

The framework \textit{Tensorflow Probability} introduce the concept of \textit{Bijectors} which are predefined (potentially non-linear) transformations of distributions. Softmax, Softplus, Sigmoid, Affine and Exponential are among many supported transformations. Using Bijectors, the proposal distributions in Tensorflow Probability can easily be transformed into a unconstrained space without dealing with any error-prone calculations \cite{tensorflow2015-whitepaper}. 


