\chapter{Analytical Methods \& Variational Inference}
For some \acrshort{pgm}'s the joint distribution may have a functional form which allow marginalization using analytical methods, i.e. directly evaluating the integrals and sums. When applicable, analytical methods allow for exact inference and avoid the randomness introduced by sampling methods. %TODO Cite or explain further 


\section{Belief Propagation}
%TODO Better citations
The computational complexity of marginalizing over many variables in a \acrshort{pgm} may be large as the number of possible hypotheses quickly increases with the number of variables. Finding the solution in a reasonable amount of time requires a strategy which avoids unnecessary computations and efficiently utilize the structure of the \acrshort{pgm}. 

\textit{\acrfull{bp}}, also called the sum-product algorithm, is an algorithm for efficient marginalization in \acrshort{pgm}'s. The algorithm utilizes dynamic programming to avoid repeating costly calculations. The method is described in detail in \cite[p .~710]{murphy}, but is summarized here with a few notational modifications. 


\subsection{Belief Propagation for trees}
To compute the marginal density for a node $r$ in a tree-structured \acrshort{pgm}, one can imagine "picking up" the tree by $r$. The remaining nodes will be pulled down by gravity and $r$ will become the root node. \acrshort{bp} will collect evidence from the leaf nodes and work its way upward toward the root, collecting evidence and marginalizing on the way. One can imagine each node computes \textbf{messages} describing the belief about their parent nodes, and passing those messages to the parent nodes \cite{murphy}.
\subsection{Message passing}
A node $t$ computes a message containing the current belief about its parent given all evidence collected at or below node $t$ \cite{murphy}. Assuming all nodes below $t$ have already calculated their messages, the belief for node $t$ becomes
\begin{equation}\label{eq:bp_belief}
    bel_t^-(x_t) \triangleq p(x_t | \mathbf{v}_t^-) = \frac{1}{Z_t}\psi_t(x_t) \prod_{c \in ch(t)} m_{c\to t}^-(x_t)
\end{equation}
where $\mathbf{v}_t^-$ is all evidence at or below node $t$, $\psi_t(x_t)$ is the local evidence at node $t$, $m_{c \to t}^-$ is the messages passed from $t$'s children, $ch(t)$ is the children nodes for node $t$ and $Z_t$ is the normalization constant for node $t$.
Assuming the belief $bel_s^-(x_s)$ for a children $s$ of node $t$ has already been calculated using \cref{eq:bp_belief},  the message $m_{s\to t}^-$ can then be computed using
\begin{equation}\label{eq:bp_message}
    m_{s \to t}^-(x_t) = \int_{x_s} \psi_{st}(x_s, x_t) bel_s^-(x_s) dx_s
\end{equation}
where $\psi_{st}(x_s, x_t)$ is the edge potential. The edge potential $\psi_{st}(x_s, x_t)$ and local evidence $\psi_t(x_t)$ here corresponds to the factor potentials discussed in \cref{sec:mrf}.

The messages and belief state for each node can then be recursively computed using \cref{eq:bp_belief} and \cref{eq:bp_message}.

\begin{subequations}
\begin{align}
p(x_r) &= \idotsint_V p(x_r, x_1,\dots,x_k) \,dx_1 \dots dx_k\\
&= \idotsint_V p(x_r | x_1)p(x_1|x_2)\dots p(x_{k-1} | x_k)p(x_k) \, dx_1 \dots dx_k\\
&= \int_{x_1} p(x_r | x_1)\underbrace{\int_{x_2}p(x1 | x2) \; \;  \dots \underbrace{\int_{x_k} p(x_{k-1} | x_k)p(x_k) \, dx_1 \dots dx_k}_{m_{k\to k-1}^-(x_{k-1})}}_{m_{x_2 \to x_1}^-(x_1)}
\end{align}
\end{subequations}

Computing all messages from leaf nodes towards the top is called the \textbf{collecting evidence}-phase and yields the marginal distribution for the root node. However, the marginal distribution for all other nodes can easily be calculated by calculating messages from the root node and distribute messages downward towards the leaf nodes. This is called the \textbf{distribute evidence}-phase. 

The belief state for nodes $s$ other than the root, can in the distribute evidence-phase be updated using
\begin{equation}\label{eq:bp_belief_top_down}
    bel_s(x_s) \triangleq p(x_s | \mathbf{v}) \propto bel_s^-(x_s) \prod_{t \in pa(s)} m_{t \to s}^+(x_s)
\end{equation}
where $\mathbf{v}$ is all evidence and $m_{t \to s}^+$ are the top-down message from $t$ to $s$, which summarizes all the remaining information in the graph about node $s$. 

The top-down messages $m_{t \to s}^+$ can be computed by combining all the evidence received by $t$, except for what $s$ sent it.

\begin{subequations}
\begin{align}
    m_{t \to s}^+(x_s) &\triangleq p(x_s | \mathbf{v}_{st}^+) = \int_{x_t}\psi_{st}(x_s, x_t)\frac{bel_t(x_t)}{m_{s \to t}^-(x_t)}dx_t \label{eq:bp_belief_updating}\\
    &= \int_{x_t} \psi_{st}(x_s, x_t)\psi_t(x_t) \prod_{c \in ch(t), c \neq s} m_{c \to t}^-(x_t) \prod_{p \in pa(t)} m_{p \to t}^+(x_t) \; dx_t\label{eq:bp_sum_product}
\end{align}
\end{subequations}

\cref{eq:bp_sum_product} is found by inserting the equation for $bel_t(x_t)$, \cref{eq:bp_belief_top_down}, into \cref{eq:bp_belief_updating}, and is called the \textit{sum-product} algorithm as it multiplies all-but-one messages and marginalizes (summing) out variables. 

While \acrshort{bp} may sound complicated, one can think of this algorithm as computing the marginal distribution of $x_r$ where each sum or integral is pushed in as far as possible to simplify calculations. By beginning with the inner-most integral, the marginalization boils down to recursively computing simple integrals or sums with only a small subset of the variables. This general method is called \textit{variable elimination}\cite{murphy}. \acrshort{bp} extends this idea by also utilizing the graph structure to reduce the computational complexity. A \acrshort{pgm} with a chain structure is used as an example to show the rather simple concept behind variable elimination and \acrshort{bp}.

\subsection{Belief Propagation on arbitrary graphs}


\subsection{Limitations of analytical methods}
The methods described so far in this chapter generally applies to both discrete and continous random variables by replacing integrals with sums for discrete variables. However, in practice, continous random variables quickly becomes intractable unless each parent-child in the graph are conjugate priors. Assuming conjugate priors severly limits the ability to describe the data-generating process, leaving only a few types of networks feasible. Looking back at \cref{table:conjugate_priors} for common conjugate priors, only a mix of gaussians, gamma and discrete distributions allow for multiple levels in a hierical model. For all other distributions, one quickly ends up in a situation where no standard conjugate exist \cite{winnbishop}. 
% TODO Cite. 
\section{Variational Bayes}
\acrshort{bp} allows for exact inference of \acrshort{pgm}'s when there exists closed-form solutions of the integrals. This is however rarely the case and is only true for carefully chosen distributions. Requiring all combination of nodes to be conjugate priors (\cref{sec:theory_conjugate_priors}) can severely limit the expressibility of a \acrshort{pgm} and is not a viable solution. 

Another approach is to approximate the target distribution $p^*(\mathbf{x} | \mathbf{v})$ with a \textbf{surrogate distribution} $q(\mathbf{x})$ constructed using conjugate priors to allow for exact inference. If $p^*(\cdot)$ and $q(\cdot)$ are similar, then \acrshort{bp} can be used on $q(\cdot)$ to do approximate inference.

\subsection{Kullback-Liebler divergence}
The \textit{\acrfull{kl}}  is used as a measure of difference between two probability distributions \cite{kullback1951,murphy}.

\begin{equation}\label{eq:kl}
    \mathbb{KL}(p^* || q) = \int_\mathbf{x} p^*(\mathbf{x}) \log \frac{p^*(\mathbf{x})}{q(\mathbf{x})} d\mathbf{x} = E_{p^*} \big[ \log \frac{p^*(\mathbf{x})}{q(\mathbf{x})} \big]
\end{equation}
The \acrshort{kl} divergence boils down to the expected difference between $p^*(\cdot)$ and $q(\cdot)$ over $p^*(\cdot)$.
In this case $p^*(\cdot)$ is only assumed known up to a normalizing constant, making the expectation in \cref{eq:kl} intractable to compute. Instead, the reverse \acrshort{kl} divergence can be used as $q(\cdot)$ is assumed known.
\begin{equation}\label{eq:reverse_kl}
    \mathbb{KL}(q || p^*) = \int_\mathbf{x} q(\mathbf{x}) \log \frac{q(\mathbf{x})}{p^*(\mathbf{x})} d\mathbf{x} = E_{q} \big[ \log \frac{q(\mathbf{x})}{p^*(\mathbf{x})} \big]
\end{equation}

\subsubsection{ELBO}
In the case of variational inference, the goal is usually to find an approximation $q(\mathbf{z})$ for the posterior $p(\mathbf{z} | \mathbf{x}$.) The reverse \acrshort{kl} can then be written as.
\begin{subequations}
\begin{align}
    \mathbb{KL}(q(\mathbf{x}) || p^*(\mathbf{x} | \mathcal{D})) &= E_q\big[\log \frac{q(\mathbf{x})}{p^*(\mathbf{x} | \mathcal{D})} \big]\label{eq:elbo_kl}\\
    &= E_q\big[\log \frac{q(\mathbf{z}) p^*(\mathbf{x})}{p^*(\mathbf{z}, \mathbf{x})} \big]\\
    &= E_q\big[\log q(\mathbf{x}) \big] - E_q\big[\log p^*(\mathbf{x}, \mathcal{D})] + E_q\big[\log p^*(\mathcal{D}) \big]\\
    &= E_q\big[\log q(\mathbf{x}) \big] - E_q\big[\log p^*(\mathbf{x}, \mathcal{D})] + \log p^*(\mathcal{D})\\
    &\leq E_q\big[\log q(\mathbf{x}) \big] - E_q\big[\log p(\mathbf{x}, \mathcal{D})]\label{eq:negative_elbo}
\end{align}
\end{subequations}

As $p(\mathcal{D})$ is assumed intractable, the reverse \acrshort{kl} cannot be evaluated. However, since the surrogate density $q(\mathbf{x})$ only depend on $\mathbf{x}$, $\mathbb{KL}(q(\mathbf{x} || p^*(\mathbf{x} | \mathcal{D}))$ can still be optimized as $p(\mathcal{D})$ is constant. In other words, minimizing \cref{eq:negative_elbo} is equivalent to minimizng \cref{eq:elbo_kl}.

This problem is usually phrased as a maximization problem, and the objective function to be \textbf{maximized} is called the \textit{\acrfull{elbo}} \cite{Blei_2017}.
\begin{equation}
    \text{ELBO}(q) = E_q\big[\log p(\mathbf{x}, \mathcal{D})] - E_q\big[\log q(\mathbf{x}) \big]
\end{equation}

Variational Inference then boils down to finding the best surrogate posterior $q^*(\mathbf{x})$ which maximizes \acrshort{elbo} or minimizing \acrshort{kl} \cite{Blei_2017}.

\begin{equation}
    q^*(\mathbf{x}) = \arg \max_{q(\mathbf{x})} \text{ELBO}(q) = \arg \min_{q(\mathbf{x})} \mathbb{KL}(q(\mathbf{x}) || p^*(\mathbf{x} | \mathcal{D}))
\end{equation}



\subsection{I or M mode projection}

Forward KL is also known as \textbf{moment-projection} (M-projection)  as it will attempt to match the moments of $q(\cdot)$ with $p^*(\cdot)$. In the case of a multimodal $p^*(\cdot)$ and unimodal $q(\cdot)$, this may not be desirable as the mean is placed inbetween regions of high probability. Looking at \cref{eq:kl}, the KL divergence is infinite when $p(\cdot) > 0$ and $q(\cdot) = 0$. M-projection is therefore said to be \textbf{zero avoiding} as optimizing \cref{eq:kl} needs to ensure $q(\mathbf{x}) > 0$ when $p(\mathbf{x}) > 0$. M-projection therefore tends to over-estimate the support of $p^*(\cdot)$.

Reverse KL is known as \textbf{information-projection} (I-projection). As \cref{eq:reverse_kl} is infinite when $p^*(\cdot) = 0$ and $q(\cdot) > 0$, optimizing reverse KL therefore needs to ensure $q(\cdot) = 0$ when $p^*(\cdot)=0$. I-projection is therefore said to be \textbf{zero forcing} and tends to underestimate the support for $p^*(\cdot)$ \cite{murphy}. 

\subsection{Mean Field}
The simplest method for variational inference is assuming independent variables for the surrogate distribution $q(\cdot)$. 
\begin{equation}\label{eq:mean_field}
    q(\mathbf{x}) = \prod_i q_i(x_i)
\end{equation}
However, this is in many cases a very strong assumption which may lead to poor results in the case of strong interactions in the posterior distribution. An extenstion called \textbf{structured mean field} can in some cases be used if the posterior has a tractable substructures (i.e. subgraphs which can be described using "simple" distributions). The variables forming tractable substructures can then be combined into "mega-variables" and \cref{eq:mean_field} can be used as an approximation.   

\subsection{Variational Message Passing}



\subsection{Calculus of Variation}